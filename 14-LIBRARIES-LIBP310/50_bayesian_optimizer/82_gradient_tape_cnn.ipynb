{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_max_value = train_x.max().astype(np.float32)\n",
    "train_x, test_x = train_x / _max_value, test_x / _max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 21:12:24.243359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-23 21:12:24.247605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-23 21:12:24.247763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-23 21:12:24.248847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-23 21:12:24.249066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-23 21:12:24.249331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-23 21:12:24.774367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-23 21:12:24.774521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-23 21:12:24.774637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-23 21:12:24.774738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6890 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y))\\\n",
    "    .shuffle(train_x.shape[0]).batch(BATCH_SIZE)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10CNNModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 filter1, kernel_size1, pool_size1, \n",
    "                 filter2, kernel_size2, pool_size2,\n",
    "                 filter3, kernel_size3, pool_size3,\n",
    "                 dense1, dense2):\n",
    "        super(Cifar10CNNModel, self).__init__(name='Cifar10CNNModel')\n",
    "        \n",
    "        self.data = Conv2D(\n",
    "            filter1, kernel_size1, activation='relu', input_shape=(train_x.shape[1:]))\n",
    "        self.mp1 = MaxPooling2D(pool_size1)\n",
    "        self.h1 = Conv2D(filter2, kernel_size2, activation='relu')\n",
    "        self.mp2 = MaxPooling2D(pool_size2)\n",
    "        self.h2 = Conv2D(filter3, kernel_size3, activation='relu')\n",
    "        self.mp3 = MaxPooling2D(pool_size3)\n",
    "        self.f = Flatten()\n",
    "        self.fc1 = Dense(dense1, activation='relu')\n",
    "        self.fc2 = Dense(dense2, activation='relu')\n",
    "        self.result = Dense(len(CLASS_NAMES), activation='softmax')\n",
    "        \n",
    "    def call(self, input):\n",
    "        model = self.data(input)\n",
    "        model = self.mp1(model)\n",
    "        model = self.h1(model)\n",
    "        model = self.mp2(model)\n",
    "        model = self.h2(model)\n",
    "        model = self.mp3(model)\n",
    "        model = self.f(model)\n",
    "        model = self.fc1(model)\n",
    "        model = self.fc2(model)\n",
    "        model = self.result(model)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels, model, optimizer, train_loss, train_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        _loss = loss(labels, predictions)\n",
    "    gradients = tape.gradient(_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(_loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels, model, test_loss, test_accuracy):\n",
    "    predictions = model(images, training=False)\n",
    "    _loss = loss(labels, predictions)\n",
    "    test_loss(_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(\n",
    "    filter1, kernel_size1, pool_size1, \n",
    "    filter2, kernel_size2, pool_size2,\n",
    "    filter3, kernel_size3, pool_size3,\n",
    "    dense1, dense2, lr\n",
    "):\n",
    "    model = Cifar10CNNModel(\n",
    "        filter1, kernel_size1, pool_size1, \n",
    "        filter2, kernel_size2, pool_size2,\n",
    "        filter3, kernel_size3, pool_size3,\n",
    "        dense1, dense2\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(lr)\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "    \n",
    "    return model, optimizer, train_loss, train_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(\n",
    "    filter1, kernel_size1, pool_size1, \n",
    "    filter2, kernel_size2, pool_size2,\n",
    "    filter3, kernel_size3, pool_size3,\n",
    "    dense1, dense2, lr, epochs\n",
    "):\n",
    "    model, optimizer, train_loss, train_accuracy, test_loss, test_accuracy = create_model(\n",
    "        filter1, kernel_size1, pool_size1, \n",
    "        filter2, kernel_size2, pool_size2,\n",
    "        filter3, kernel_size3, pool_size3,\n",
    "        dense1, dense2, lr\n",
    "    )\n",
    "    for epoch in range(epochs):\n",
    "        train_loss.reset_state()\n",
    "        train_accuracy.reset_state()\n",
    "        test_loss.reset_state()\n",
    "        test_accuracy.reset_state()\n",
    "        \n",
    "        for _images, _labels in train_ds:\n",
    "            train_step(_images, _labels, model, optimizer)\n",
    "            \n",
    "        for _images, _labels in test_ds:\n",
    "            test_step(_images, _labels, model)\n",
    "            \n",
    "    return test_accuracy.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_func(\n",
    "    filter1, kernel_size1, pool_size1, \n",
    "    filter2, kernel_size2, pool_size2,\n",
    "    filter3, kernel_size3, pool_size3,\n",
    "    dense1, dense2, lr, epochs\n",
    "):\n",
    "    return training(\n",
    "        int(filter1), int(kernel_size1), int(pool_size1), \n",
    "        int(filter2), int(kernel_size2), int(pool_size2),\n",
    "        int(filter3), int(kernel_size3), int(pool_size3),\n",
    "        int(dense1), int(dense2), lr, int(epochs)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'filter1': (32, 128), 'kernel_size1': (1,5), 'pool_size1': (1,5),\n",
    "    'filter2': (64, 256), 'kernel_size2': (1,5), 'pool_size2': (1,5),\n",
    "    'filter3': (128, 512), 'kernel_size3': (1,5), 'pool_size3': (1,5),\n",
    "    'dense1': (256, 512), 'dense2': (128, 256), \n",
    "    'lr': (1e-4, 1e-2), 'epochs': (5, 15)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_optimizer = BayesianOptimization(\n",
    "    f=None,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,\n",
    "    random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility = UtilityFunction(\n",
    "    kind='ucb', # 탐색과 활용 사이의 균형 유지, 불확실한 지점을 더 많이 탐색\n",
    "    kappa=2.5,  # UCB전략에서 얼마의 불확실성을 고려할지 지정\n",
    "                # 값이 높으면 탐색을, 값이 낮으면 활용을 강조함\n",
    "    xi=0.0      # 탐색전략에서 사용되는 파라미터로 얼마나 큰 개선을 고려할지 조정\n",
    "                # 값이 높으면 큰 개선을 값이 낮으면 작은 개선을 탐색함\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dense1': 434.29611151305255, 'dense2': 164.62583487364856, 'epochs': 7.268514535642031, 'filter1': 84.92621783195756, 'filter2': 202.13804219882812, 'filter3': 290.472880687793, 'kernel_size1': 4.923056793538462, 'kernel_size2': 3.739318954339453, 'kernel_size3': 2.9237276059374437, 'lr': 0.0039819634301220905, 'pool_size1': 2.3727120646034776, 'pool_size2': 3.9161988295361665, 'pool_size3': 2.7542889787184976}\n"
     ]
    }
   ],
   "source": [
    "next_point = bo_optimizer.suggest(utility)\n",
    "print(next_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 22:08:49.946115: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [50000,1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-08-23 22:08:49.946375: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [50000,1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-08-23 22:08:50.990555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8904\n",
      "2023-08-23 22:08:51.406234: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-08-23 22:08:51.767963: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fbcbe775ab0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-23 22:08:51.767985: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1\n",
      "2023-08-23 22:08:51.771422: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-23 22:08:51.833841: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-08-23 22:08:51.878264: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-08-23 22:08:59.025484: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [10000,1]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5036\n",
      "처리시간: 0:00:41.878946\n"
     ]
    }
   ],
   "source": [
    "_start = datetime.now()\n",
    "target = target_func(**next_point)\n",
    "print(target)\n",
    "print(f'처리시간: {datetime.now()-_start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_optimizer.register(params=next_point, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_7938/2119553101.py\", line 4, in train_step  *\n        predictions = model(images, training=True)\n    File \"/home/freeman/anaconda3/envs/t212p38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file_aysxdpj.py\", line 10, in tf__call\n        model = ag__.converted_call(ag__.ld(self).data, (ag__.ld(input),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'Cifar10CNNModel' (type Cifar10CNNModel).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_7938/2134668432.py\", line 23, in call  *\n            model = self.data(input)\n        File \"/home/freeman/anaconda3/envs/t212p38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n    \n    \n    Call arguments received by layer 'Cifar10CNNModel' (type Cifar10CNNModel):\n      • input=tf.Tensor(shape=(32, 32, 32, 3), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m __start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m      4\u001b[0m next_point \u001b[39m=\u001b[39m bo_optimizer\u001b[39m.\u001b[39msuggest(utility)\n\u001b[0;32m----> 5\u001b[0m target \u001b[39m=\u001b[39m target_func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnext_point)\n\u001b[1;32m      6\u001b[0m bo_optimizer\u001b[39m.\u001b[39mregister(params\u001b[39m=\u001b[39mnext_point, target\u001b[39m=\u001b[39mtarget)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m      8\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39midx(\u001b[39m\u001b[39m{\u001b[39;00mloop\u001b[39m:\u001b[39;00m\u001b[39m>2\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m-\u001b[39m__start\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy: \u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m, params: \u001b[39m\u001b[39m{\u001b[39;00mnext_point\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m, in \u001b[0;36mtarget_func\u001b[0;34m(filter1, kernel_size1, pool_size1, filter2, kernel_size2, pool_size2, filter3, kernel_size3, pool_size3, dense1, dense2, lr, epochs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtarget_func\u001b[39m(\n\u001b[1;32m      2\u001b[0m     filter1, kernel_size1, pool_size1, \n\u001b[1;32m      3\u001b[0m     filter2, kernel_size2, pool_size2,\n\u001b[1;32m      4\u001b[0m     filter3, kernel_size3, pool_size3,\n\u001b[1;32m      5\u001b[0m     dense1, dense2, lr, epochs\n\u001b[1;32m      6\u001b[0m ):\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mreturn\u001b[39;00m training(\n\u001b[1;32m      8\u001b[0m         \u001b[39mint\u001b[39;49m(filter1), \u001b[39mint\u001b[39;49m(kernel_size1), \u001b[39mint\u001b[39;49m(pool_size1), \n\u001b[1;32m      9\u001b[0m         \u001b[39mint\u001b[39;49m(filter2), \u001b[39mint\u001b[39;49m(kernel_size2), \u001b[39mint\u001b[39;49m(pool_size2),\n\u001b[1;32m     10\u001b[0m         \u001b[39mint\u001b[39;49m(filter3), \u001b[39mint\u001b[39;49m(kernel_size3), \u001b[39mint\u001b[39;49m(pool_size3),\n\u001b[1;32m     11\u001b[0m         \u001b[39mint\u001b[39;49m(dense1), \u001b[39mint\u001b[39;49m(dense2), lr, \u001b[39mint\u001b[39;49m(epochs)\n\u001b[1;32m     12\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(filter1, kernel_size1, pool_size1, filter2, kernel_size2, pool_size2, filter3, kernel_size3, pool_size3, dense1, dense2, lr, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m test_accuracy\u001b[39m.\u001b[39mreset_state()\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m _images, _labels \u001b[39min\u001b[39;00m train_ds:\n\u001b[0;32m---> 20\u001b[0m     train_step(_images, _labels, model, optimizer)\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m _images, _labels \u001b[39min\u001b[39;00m test_ds:\n\u001b[1;32m     23\u001b[0m     test_step(_images, _labels, model)\n",
      "File \u001b[0;32m~/anaconda3/envs/t212p38/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file89ztcqpc.py:9\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(images, labels, model, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mFunctionScope(\u001b[39m'\u001b[39m\u001b[39mtrain_step\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfscope\u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mConversionOptions(recursive\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, user_requested\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, optional_features\u001b[39m=\u001b[39m(), internal_convert_user_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)) \u001b[39mas\u001b[39;00m fscope:\n\u001b[1;32m      8\u001b[0m     \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m----> 9\u001b[0m         predictions \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(model), (ag__\u001b[39m.\u001b[39mld(images),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)\n\u001b[1;32m     10\u001b[0m         _loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(loss), (ag__\u001b[39m.\u001b[39mld(labels), ag__\u001b[39m.\u001b[39mld(predictions)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m     gradients \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tape)\u001b[39m.\u001b[39mgradient, (ag__\u001b[39m.\u001b[39mld(_loss), ag__\u001b[39m.\u001b[39mld(model)\u001b[39m.\u001b[39mtrainable_variables), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m~/anaconda3/envs/t212p38/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_aysxdpj.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m model \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdata, (ag__\u001b[39m.\u001b[39mld(\u001b[39minput\u001b[39m),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mmp1, (ag__\u001b[39m.\u001b[39mld(model),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m model \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mh1, (ag__\u001b[39m.\u001b[39mld(model),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_7938/2119553101.py\", line 4, in train_step  *\n        predictions = model(images, training=True)\n    File \"/home/freeman/anaconda3/envs/t212p38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file_aysxdpj.py\", line 10, in tf__call\n        model = ag__.converted_call(ag__.ld(self).data, (ag__.ld(input),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'Cifar10CNNModel' (type Cifar10CNNModel).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_7938/2134668432.py\", line 23, in call  *\n            model = self.data(input)\n        File \"/home/freeman/anaconda3/envs/t212p38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n    \n    \n    Call arguments received by layer 'Cifar10CNNModel' (type Cifar10CNNModel):\n      • input=tf.Tensor(shape=(32, 32, 32, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "_start = datetime.now()\n",
    "for loop in range(20):\n",
    "    __start = datetime.now()\n",
    "    next_point = bo_optimizer.suggest(utility)\n",
    "    target = target_func(**next_point)\n",
    "    bo_optimizer.register(params=next_point, target=target)\n",
    "    print(\n",
    "        f'idx({loop:>2}): {datetime.now()-__start}, '\n",
    "        f'accuracy: {target}, params: {next_point}'\n",
    "    )\n",
    "print('--------------')\n",
    "print(bo_optimizer.max)\n",
    "print(f'처리시간: {datetime.now() - _start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t212p38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
