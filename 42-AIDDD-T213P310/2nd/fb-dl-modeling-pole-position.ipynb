{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-17 16:21:11.005214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from freeman.plt_setting import plt_settings\n",
    "from freeman.evaluation import regression_evaluation, f_importances, plot_actual_pred\n",
    "from freeman.aiddd.data_manager import read_data\n",
    "\n",
    "# 한글처리 지원\n",
    "plt_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = read_data('2nd pp pole-position-on-cons-1st')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = df_data.select_dtypes(include=['number']).columns.tolist()\n",
    "target_column = '총공사비'\n",
    "feature_columns.remove(target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df_data[feature_columns + [target_column]]\n",
    "df_y = df_X.pop(target_column)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(df_X, df_y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "X_train_scaled = standard_scaler.fit_transform(X_train)\n",
    "X_test_scaled = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-17 16:22:09.049929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-17 16:22:09.063248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-17 16:22:09.063438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-17 16:22:09.064675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-17 16:22:09.064891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-17 16:22:09.065018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-17 16:22:10.307864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-17 16:22:10.308023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-17 16:22:10.308145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-17 16:22:10.308252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6789 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               4224      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14593 (57.00 KB)\n",
      "Trainable params: 14593 (57.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1:])),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model_mlp.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-17 16:22:21.990229: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e373950 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-17 16:22:21.990268: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1\n",
      "2023-09-17 16:22:22.045913: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-17 16:22:22.350644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2023-09-17 16:22:22.491714: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-09-17 16:22:22.607232: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 - 3s - loss: 57255940587520.0000 - mae: 5338034.0000 - val_loss: 60902787252224.0000 - val_mae: 5457325.5000 - 3s/epoch - 10ms/step\n",
      "Epoch 2/200\n",
      "341/341 - 1s - loss: 49541202051072.0000 - mae: 4951820.5000 - val_loss: 42921575317504.0000 - val_mae: 4512971.0000 - 672ms/epoch - 2ms/step\n",
      "Epoch 3/200\n",
      "341/341 - 1s - loss: 29140216446976.0000 - mae: 3569555.2500 - val_loss: 25494638559232.0000 - val_mae: 3128543.7500 - 664ms/epoch - 2ms/step\n",
      "Epoch 4/200\n",
      "341/341 - 1s - loss: 20345849380864.0000 - mae: 2639769.2500 - val_loss: 20887812177920.0000 - val_mae: 2432304.7500 - 659ms/epoch - 2ms/step\n",
      "Epoch 5/200\n",
      "341/341 - 1s - loss: 16981911666688.0000 - mae: 2031841.6250 - val_loss: 18153365045248.0000 - val_mae: 1955061.2500 - 658ms/epoch - 2ms/step\n",
      "Epoch 6/200\n",
      "341/341 - 1s - loss: 15095231938560.0000 - mae: 1793065.6250 - val_loss: 16862232444928.0000 - val_mae: 1885700.8750 - 666ms/epoch - 2ms/step\n",
      "Epoch 7/200\n",
      "341/341 - 1s - loss: 14286011236352.0000 - mae: 1815956.5000 - val_loss: 16297190490112.0000 - val_mae: 1949942.0000 - 665ms/epoch - 2ms/step\n",
      "Epoch 8/200\n",
      "341/341 - 1s - loss: 13929661071360.0000 - mae: 1862208.1250 - val_loss: 16027982233600.0000 - val_mae: 2008650.2500 - 685ms/epoch - 2ms/step\n",
      "Epoch 9/200\n",
      "341/341 - 1s - loss: 13723727036416.0000 - mae: 1867768.2500 - val_loss: 15865772769280.0000 - val_mae: 2008673.1250 - 691ms/epoch - 2ms/step\n",
      "Epoch 10/200\n",
      "341/341 - 1s - loss: 13578747772928.0000 - mae: 1859605.7500 - val_loss: 15753905438720.0000 - val_mae: 1999582.0000 - 678ms/epoch - 2ms/step\n",
      "Epoch 11/200\n",
      "341/341 - 1s - loss: 13478343475200.0000 - mae: 1857562.8750 - val_loss: 15666418548736.0000 - val_mae: 1961764.8750 - 668ms/epoch - 2ms/step\n",
      "Epoch 12/200\n",
      "341/341 - 1s - loss: 13391314812928.0000 - mae: 1836487.2500 - val_loss: 15604017790976.0000 - val_mae: 1960372.6250 - 717ms/epoch - 2ms/step\n",
      "Epoch 13/200\n",
      "341/341 - 1s - loss: 13314545418240.0000 - mae: 1818852.6250 - val_loss: 15564479135744.0000 - val_mae: 1942767.8750 - 663ms/epoch - 2ms/step\n",
      "Epoch 14/200\n",
      "341/341 - 1s - loss: 13265099816960.0000 - mae: 1815288.1250 - val_loss: 15532489179136.0000 - val_mae: 1921315.3750 - 679ms/epoch - 2ms/step\n",
      "Epoch 15/200\n",
      "341/341 - 1s - loss: 13218812526592.0000 - mae: 1796294.5000 - val_loss: 15504722886656.0000 - val_mae: 1923635.7500 - 675ms/epoch - 2ms/step\n",
      "Epoch 16/200\n",
      "341/341 - 1s - loss: 13192842444800.0000 - mae: 1801511.5000 - val_loss: 15493774704640.0000 - val_mae: 1893014.8750 - 680ms/epoch - 2ms/step\n",
      "Epoch 17/200\n",
      "341/341 - 1s - loss: 13161768943616.0000 - mae: 1783709.2500 - val_loss: 15477922332672.0000 - val_mae: 1895336.0000 - 702ms/epoch - 2ms/step\n",
      "Epoch 18/200\n",
      "341/341 - 1s - loss: 13133264453632.0000 - mae: 1775920.3750 - val_loss: 15469091225600.0000 - val_mae: 1886297.6250 - 705ms/epoch - 2ms/step\n",
      "Epoch 19/200\n",
      "341/341 - 1s - loss: 13117074440192.0000 - mae: 1777535.2500 - val_loss: 15458003582976.0000 - val_mae: 1881865.2500 - 659ms/epoch - 2ms/step\n",
      "Epoch 20/200\n",
      "341/341 - 1s - loss: 13092369989632.0000 - mae: 1761895.1250 - val_loss: 15444945666048.0000 - val_mae: 1888916.8750 - 679ms/epoch - 2ms/step\n",
      "Epoch 21/200\n",
      "341/341 - 1s - loss: 13075414515712.0000 - mae: 1769688.0000 - val_loss: 15440364437504.0000 - val_mae: 1877671.2500 - 695ms/epoch - 2ms/step\n",
      "Epoch 22/200\n",
      "341/341 - 1s - loss: 13059707895808.0000 - mae: 1763366.8750 - val_loss: 15454106025984.0000 - val_mae: 1856919.5000 - 672ms/epoch - 2ms/step\n",
      "Epoch 23/200\n",
      "341/341 - 1s - loss: 13044207845376.0000 - mae: 1759855.8750 - val_loss: 15439030648832.0000 - val_mae: 1863805.1250 - 661ms/epoch - 2ms/step\n",
      "Epoch 24/200\n",
      "341/341 - 1s - loss: 13030449479680.0000 - mae: 1748217.2500 - val_loss: 15423077613568.0000 - val_mae: 1882537.1250 - 675ms/epoch - 2ms/step\n",
      "Epoch 25/200\n",
      "341/341 - 1s - loss: 13014442967040.0000 - mae: 1758916.7500 - val_loss: 15418600194048.0000 - val_mae: 1880881.6250 - 677ms/epoch - 2ms/step\n",
      "Epoch 26/200\n",
      "341/341 - 1s - loss: 13001462644736.0000 - mae: 1754353.8750 - val_loss: 15408942809088.0000 - val_mae: 1868165.1250 - 689ms/epoch - 2ms/step\n",
      "Epoch 27/200\n",
      "341/341 - 1s - loss: 12991910117376.0000 - mae: 1740718.2500 - val_loss: 15401133015040.0000 - val_mae: 1874048.3750 - 664ms/epoch - 2ms/step\n",
      "Epoch 28/200\n",
      "341/341 - 1s - loss: 12979013681152.0000 - mae: 1742150.2500 - val_loss: 15403977801728.0000 - val_mae: 1916738.5000 - 668ms/epoch - 2ms/step\n",
      "Epoch 29/200\n",
      "341/341 - 1s - loss: 12958539186176.0000 - mae: 1745211.6250 - val_loss: 15407791472640.0000 - val_mae: 1928936.0000 - 669ms/epoch - 2ms/step\n",
      "Epoch 30/200\n",
      "341/341 - 1s - loss: 12969422356480.0000 - mae: 1769592.5000 - val_loss: 15380203438080.0000 - val_mae: 1850942.2500 - 677ms/epoch - 2ms/step\n",
      "Epoch 31/200\n",
      "341/341 - 1s - loss: 12948683620352.0000 - mae: 1740589.7500 - val_loss: 15376659251200.0000 - val_mae: 1856368.3750 - 674ms/epoch - 2ms/step\n",
      "Epoch 32/200\n",
      "341/341 - 1s - loss: 12933811666944.0000 - mae: 1739878.3750 - val_loss: 15367771521024.0000 - val_mae: 1887677.8750 - 697ms/epoch - 2ms/step\n",
      "Epoch 33/200\n",
      "341/341 - 1s - loss: 12926347902976.0000 - mae: 1749630.5000 - val_loss: 15359379767296.0000 - val_mae: 1856037.2500 - 660ms/epoch - 2ms/step\n",
      "Epoch 34/200\n",
      "341/341 - 1s - loss: 12921149063168.0000 - mae: 1729337.7500 - val_loss: 15357172514816.0000 - val_mae: 1887111.7500 - 655ms/epoch - 2ms/step\n",
      "Epoch 35/200\n",
      "341/341 - 1s - loss: 12891075903488.0000 - mae: 1750028.3750 - val_loss: 15363633840128.0000 - val_mae: 1834859.3750 - 660ms/epoch - 2ms/step\n",
      "Epoch 36/200\n",
      "341/341 - 1s - loss: 12882385305600.0000 - mae: 1722174.0000 - val_loss: 15350681829376.0000 - val_mae: 1911205.3750 - 659ms/epoch - 2ms/step\n",
      "Epoch 37/200\n",
      "341/341 - 1s - loss: 12890615578624.0000 - mae: 1749061.0000 - val_loss: 15328342966272.0000 - val_mae: 1881162.7500 - 667ms/epoch - 2ms/step\n",
      "Epoch 38/200\n",
      "341/341 - 1s - loss: 12876706217984.0000 - mae: 1739113.0000 - val_loss: 15321410830336.0000 - val_mae: 1888204.5000 - 816ms/epoch - 2ms/step\n",
      "Epoch 39/200\n",
      "341/341 - 1s - loss: 12868886986752.0000 - mae: 1738645.6250 - val_loss: 15317148368896.0000 - val_mae: 1887605.8750 - 697ms/epoch - 2ms/step\n",
      "Epoch 40/200\n",
      "341/341 - 1s - loss: 12845922123776.0000 - mae: 1750731.8750 - val_loss: 15315464355840.0000 - val_mae: 1837350.7500 - 680ms/epoch - 2ms/step\n",
      "Epoch 41/200\n",
      "341/341 - 1s - loss: 12853124792320.0000 - mae: 1724725.8750 - val_loss: 15295698698240.0000 - val_mae: 1875535.0000 - 676ms/epoch - 2ms/step\n",
      "Epoch 42/200\n",
      "341/341 - 1s - loss: 12844658589696.0000 - mae: 1744779.6250 - val_loss: 15296391806976.0000 - val_mae: 1850457.6250 - 676ms/epoch - 2ms/step\n",
      "Epoch 43/200\n",
      "341/341 - 1s - loss: 12829620961280.0000 - mae: 1746255.5000 - val_loss: 15289253101568.0000 - val_mae: 1854254.0000 - 663ms/epoch - 2ms/step\n",
      "Epoch 44/200\n",
      "341/341 - 1s - loss: 12829830676480.0000 - mae: 1726249.2500 - val_loss: 15282067210240.0000 - val_mae: 1868987.2500 - 672ms/epoch - 2ms/step\n",
      "Epoch 45/200\n",
      "341/341 - 1s - loss: 12818757713920.0000 - mae: 1732691.8750 - val_loss: 15271517487104.0000 - val_mae: 1871996.3750 - 680ms/epoch - 2ms/step\n",
      "Epoch 46/200\n",
      "341/341 - 1s - loss: 12813230669824.0000 - mae: 1742496.2500 - val_loss: 15272790458368.0000 - val_mae: 1854969.7500 - 677ms/epoch - 2ms/step\n",
      "Epoch 47/200\n",
      "341/341 - 1s - loss: 12800835452928.0000 - mae: 1728732.7500 - val_loss: 15267494100992.0000 - val_mae: 1879566.6250 - 658ms/epoch - 2ms/step\n",
      "Epoch 48/200\n",
      "341/341 - 1s - loss: 12803620470784.0000 - mae: 1740055.8750 - val_loss: 15257094324224.0000 - val_mae: 1858880.2500 - 690ms/epoch - 2ms/step\n",
      "Epoch 49/200\n",
      "341/341 - 1s - loss: 12787325599744.0000 - mae: 1738916.1250 - val_loss: 15252288700416.0000 - val_mae: 1857416.1250 - 655ms/epoch - 2ms/step\n",
      "Epoch 50/200\n",
      "341/341 - 1s - loss: 12786498273280.0000 - mae: 1724408.1250 - val_loss: 15240409382912.0000 - val_mae: 1866061.1250 - 672ms/epoch - 2ms/step\n",
      "Epoch 51/200\n",
      "341/341 - 1s - loss: 12775906607104.0000 - mae: 1733781.7500 - val_loss: 15238080495616.0000 - val_mae: 1877568.8750 - 664ms/epoch - 2ms/step\n",
      "Epoch 52/200\n",
      "341/341 - 1s - loss: 12778847862784.0000 - mae: 1736055.0000 - val_loss: 15230619877376.0000 - val_mae: 1860421.8750 - 655ms/epoch - 2ms/step\n",
      "Epoch 53/200\n",
      "341/341 - 1s - loss: 12762265681920.0000 - mae: 1730678.5000 - val_loss: 15224249778176.0000 - val_mae: 1859938.6250 - 663ms/epoch - 2ms/step\n",
      "Epoch 54/200\n",
      "341/341 - 1s - loss: 12761067159552.0000 - mae: 1731298.3750 - val_loss: 15224970149888.0000 - val_mae: 1848868.6250 - 672ms/epoch - 2ms/step\n",
      "Epoch 55/200\n",
      "341/341 - 1s - loss: 12747005755392.0000 - mae: 1737747.6250 - val_loss: 15219057229824.0000 - val_mae: 1851649.1250 - 654ms/epoch - 2ms/step\n",
      "Epoch 56/200\n",
      "341/341 - 1s - loss: 12734858002432.0000 - mae: 1738328.3750 - val_loss: 15234324496384.0000 - val_mae: 1827429.2500 - 659ms/epoch - 2ms/step\n",
      "Epoch 57/200\n",
      "341/341 - 1s - loss: 12736062816256.0000 - mae: 1717706.2500 - val_loss: 15207785037824.0000 - val_mae: 1880074.8750 - 658ms/epoch - 2ms/step\n",
      "Epoch 58/200\n",
      "341/341 - 1s - loss: 12736175013888.0000 - mae: 1732198.8750 - val_loss: 15196233924608.0000 - val_mae: 1875833.7500 - 663ms/epoch - 2ms/step\n",
      "Epoch 59/200\n",
      "341/341 - 1s - loss: 12728350539776.0000 - mae: 1736433.8750 - val_loss: 15190805446656.0000 - val_mae: 1863862.0000 - 660ms/epoch - 2ms/step\n",
      "Epoch 60/200\n",
      "341/341 - 1s - loss: 12721725636608.0000 - mae: 1726141.1250 - val_loss: 15192811372544.0000 - val_mae: 1856375.7500 - 658ms/epoch - 2ms/step\n",
      "Epoch 61/200\n",
      "341/341 - 1s - loss: 12716683034624.0000 - mae: 1734930.2500 - val_loss: 15180383649792.0000 - val_mae: 1845056.5000 - 678ms/epoch - 2ms/step\n",
      "Epoch 62/200\n",
      "341/341 - 1s - loss: 12706171060224.0000 - mae: 1718173.6250 - val_loss: 15173154766848.0000 - val_mae: 1864146.7500 - 659ms/epoch - 2ms/step\n",
      "Epoch 63/200\n",
      "341/341 - 1s - loss: 12704374849536.0000 - mae: 1729714.0000 - val_loss: 15170635038720.0000 - val_mae: 1851009.3750 - 674ms/epoch - 2ms/step\n",
      "Epoch 64/200\n",
      "341/341 - 1s - loss: 12699184398336.0000 - mae: 1727983.1250 - val_loss: 15167263866880.0000 - val_mae: 1860189.0000 - 688ms/epoch - 2ms/step\n",
      "Epoch 65/200\n",
      "341/341 - 1s - loss: 12693571371008.0000 - mae: 1736950.8750 - val_loss: 15171678371840.0000 - val_mae: 1837408.0000 - 674ms/epoch - 2ms/step\n",
      "Epoch 66/200\n",
      "341/341 - 1s - loss: 12686572126208.0000 - mae: 1722735.5000 - val_loss: 15156496039936.0000 - val_mae: 1866579.1250 - 740ms/epoch - 2ms/step\n",
      "Epoch 67/200\n",
      "341/341 - 1s - loss: 12685932494848.0000 - mae: 1735669.2500 - val_loss: 15153453072384.0000 - val_mae: 1851249.0000 - 686ms/epoch - 2ms/step\n",
      "Epoch 68/200\n",
      "341/341 - 1s - loss: 12670502699008.0000 - mae: 1718201.1250 - val_loss: 15147930222592.0000 - val_mae: 1879749.7500 - 659ms/epoch - 2ms/step\n",
      "Epoch 69/200\n",
      "341/341 - 1s - loss: 12675520135168.0000 - mae: 1732655.2500 - val_loss: 15144792883200.0000 - val_mae: 1857647.1250 - 663ms/epoch - 2ms/step\n",
      "Epoch 70/200\n",
      "341/341 - 1s - loss: 12663164764160.0000 - mae: 1723406.3750 - val_loss: 15129047465984.0000 - val_mae: 1885250.7500 - 666ms/epoch - 2ms/step\n",
      "Epoch 71/200\n",
      "341/341 - 1s - loss: 12664910643200.0000 - mae: 1730716.7500 - val_loss: 15131529445376.0000 - val_mae: 1862875.1250 - 666ms/epoch - 2ms/step\n",
      "Epoch 72/200\n",
      "341/341 - 1s - loss: 12648144961536.0000 - mae: 1741130.0000 - val_loss: 15155057393664.0000 - val_mae: 1821314.8750 - 675ms/epoch - 2ms/step\n",
      "Epoch 73/200\n",
      "341/341 - 1s - loss: 12649273229312.0000 - mae: 1715457.8750 - val_loss: 15124107624448.0000 - val_mae: 1865212.7500 - 671ms/epoch - 2ms/step\n",
      "Epoch 74/200\n",
      "341/341 - 1s - loss: 12642873769984.0000 - mae: 1731167.1250 - val_loss: 15117333823488.0000 - val_mae: 1862875.8750 - 655ms/epoch - 2ms/step\n",
      "Epoch 75/200\n",
      "341/341 - 1s - loss: 12634023788544.0000 - mae: 1729049.3750 - val_loss: 15127050977280.0000 - val_mae: 1843913.5000 - 653ms/epoch - 2ms/step\n",
      "Epoch 76/200\n",
      "341/341 - 1s - loss: 12626633424896.0000 - mae: 1716687.1250 - val_loss: 15119121645568.0000 - val_mae: 1871465.5000 - 664ms/epoch - 2ms/step\n",
      "Epoch 77/200\n",
      "341/341 - 1s - loss: 12630188097536.0000 - mae: 1727452.1250 - val_loss: 15109550243840.0000 - val_mae: 1868775.6250 - 727ms/epoch - 2ms/step\n",
      "Epoch 78/200\n",
      "341/341 - 1s - loss: 12620073533440.0000 - mae: 1726921.3750 - val_loss: 15112462139392.0000 - val_mae: 1884078.7500 - 690ms/epoch - 2ms/step\n",
      "Epoch 79/200\n",
      "341/341 - 1s - loss: 12619264032768.0000 - mae: 1733784.7500 - val_loss: 15102870814720.0000 - val_mae: 1845065.6250 - 661ms/epoch - 2ms/step\n",
      "Epoch 80/200\n",
      "341/341 - 1s - loss: 12618638032896.0000 - mae: 1718432.7500 - val_loss: 15098572701696.0000 - val_mae: 1867224.1250 - 681ms/epoch - 2ms/step\n",
      "Epoch 81/200\n",
      "341/341 - 1s - loss: 12610168684544.0000 - mae: 1730142.2500 - val_loss: 15107349282816.0000 - val_mae: 1840513.1250 - 664ms/epoch - 2ms/step\n",
      "Epoch 82/200\n",
      "341/341 - 1s - loss: 12606427365376.0000 - mae: 1727847.7500 - val_loss: 15096256397312.0000 - val_mae: 1844929.0000 - 713ms/epoch - 2ms/step\n",
      "Epoch 83/200\n",
      "341/341 - 1s - loss: 12602062143488.0000 - mae: 1716155.7500 - val_loss: 15086903099392.0000 - val_mae: 1855714.8750 - 714ms/epoch - 2ms/step\n",
      "Epoch 84/200\n",
      "341/341 - 1s - loss: 12599724867584.0000 - mae: 1724890.8750 - val_loss: 15083717525504.0000 - val_mae: 1863523.0000 - 670ms/epoch - 2ms/step\n",
      "Epoch 85/200\n",
      "341/341 - 1s - loss: 12591927656448.0000 - mae: 1728472.0000 - val_loss: 15079072333824.0000 - val_mae: 1859896.2500 - 676ms/epoch - 2ms/step\n",
      "Epoch 86/200\n",
      "341/341 - 1s - loss: 12588387663872.0000 - mae: 1723087.1250 - val_loss: 15073608204288.0000 - val_mae: 1853294.3750 - 664ms/epoch - 2ms/step\n",
      "Epoch 87/200\n",
      "341/341 - 1s - loss: 12578998714368.0000 - mae: 1721161.8750 - val_loss: 15071788924928.0000 - val_mae: 1865216.0000 - 670ms/epoch - 2ms/step\n",
      "Epoch 88/200\n",
      "341/341 - 1s - loss: 12576343719936.0000 - mae: 1715213.1250 - val_loss: 15078293241856.0000 - val_mae: 1880358.8750 - 655ms/epoch - 2ms/step\n",
      "Epoch 89/200\n",
      "341/341 - 1s - loss: 12583984693248.0000 - mae: 1726805.6250 - val_loss: 15066600570880.0000 - val_mae: 1855695.5000 - 684ms/epoch - 2ms/step\n",
      "Epoch 90/200\n",
      "341/341 - 1s - loss: 12578325528576.0000 - mae: 1717577.2500 - val_loss: 15060900511744.0000 - val_mae: 1879022.7500 - 703ms/epoch - 2ms/step\n",
      "Epoch 91/200\n",
      "341/341 - 1s - loss: 12576024952832.0000 - mae: 1734245.6250 - val_loss: 15059320307712.0000 - val_mae: 1864750.2500 - 667ms/epoch - 2ms/step\n",
      "Epoch 92/200\n",
      "341/341 - 1s - loss: 12560132734976.0000 - mae: 1722107.3750 - val_loss: 15061911339008.0000 - val_mae: 1872158.1250 - 681ms/epoch - 2ms/step\n",
      "Epoch 93/200\n",
      "341/341 - 1s - loss: 12558555676672.0000 - mae: 1729147.5000 - val_loss: 15057619517440.0000 - val_mae: 1839887.6250 - 654ms/epoch - 2ms/step\n",
      "Epoch 94/200\n",
      "341/341 - 1s - loss: 12556688162816.0000 - mae: 1715593.0000 - val_loss: 15050660118528.0000 - val_mae: 1856072.7500 - 725ms/epoch - 2ms/step\n",
      "Epoch 95/200\n",
      "341/341 - 1s - loss: 12552347058176.0000 - mae: 1717386.8750 - val_loss: 15047555284992.0000 - val_mae: 1871305.2500 - 658ms/epoch - 2ms/step\n",
      "Epoch 96/200\n",
      "341/341 - 1s - loss: 12554652876800.0000 - mae: 1729584.7500 - val_loss: 15042953084928.0000 - val_mae: 1859548.1250 - 660ms/epoch - 2ms/step\n",
      "Epoch 97/200\n",
      "341/341 - 1s - loss: 12552013611008.0000 - mae: 1720258.3750 - val_loss: 15043774119936.0000 - val_mae: 1870858.7500 - 683ms/epoch - 2ms/step\n",
      "Epoch 98/200\n",
      "341/341 - 1s - loss: 12541614882816.0000 - mae: 1727500.1250 - val_loss: 15035976908800.0000 - val_mae: 1853693.1250 - 677ms/epoch - 2ms/step\n",
      "Epoch 99/200\n",
      "341/341 - 1s - loss: 12538409385984.0000 - mae: 1716711.3750 - val_loss: 15036434087936.0000 - val_mae: 1856627.3750 - 721ms/epoch - 2ms/step\n",
      "Epoch 100/200\n",
      "341/341 - 1s - loss: 12542721130496.0000 - mae: 1719559.1250 - val_loss: 15032771411968.0000 - val_mae: 1864003.1250 - 675ms/epoch - 2ms/step\n",
      "Epoch 101/200\n",
      "341/341 - 1s - loss: 12527369977856.0000 - mae: 1719227.6250 - val_loss: 15045097422848.0000 - val_mae: 1838604.2500 - 674ms/epoch - 2ms/step\n",
      "Epoch 102/200\n",
      "341/341 - 1s - loss: 12522268655616.0000 - mae: 1727256.2500 - val_loss: 15059454525440.0000 - val_mae: 1824748.2500 - 682ms/epoch - 2ms/step\n",
      "Epoch 103/200\n",
      "341/341 - 1s - loss: 12511463079936.0000 - mae: 1704982.6250 - val_loss: 15034350567424.0000 - val_mae: 1883506.8750 - 730ms/epoch - 2ms/step\n",
      "Epoch 104/200\n",
      "341/341 - 1s - loss: 12518127828992.0000 - mae: 1736197.8750 - val_loss: 15029329985536.0000 - val_mae: 1837460.1250 - 677ms/epoch - 2ms/step\n",
      "Epoch 105/200\n",
      "341/341 - 1s - loss: 12512428818432.0000 - mae: 1715216.2500 - val_loss: 15020643581952.0000 - val_mae: 1868706.2500 - 682ms/epoch - 2ms/step\n",
      "Epoch 106/200\n",
      "341/341 - 1s - loss: 12513776238592.0000 - mae: 1710036.6250 - val_loss: 15020582764544.0000 - val_mae: 1856837.1250 - 733ms/epoch - 2ms/step\n",
      "Epoch 107/200\n",
      "341/341 - 1s - loss: 12509576691712.0000 - mae: 1717604.5000 - val_loss: 15021463568384.0000 - val_mae: 1880907.0000 - 695ms/epoch - 2ms/step\n",
      "Epoch 108/200\n",
      "341/341 - 1s - loss: 12508794454016.0000 - mae: 1728124.5000 - val_loss: 15014758973440.0000 - val_mae: 1854993.5000 - 672ms/epoch - 2ms/step\n",
      "Epoch 109/200\n",
      "341/341 - 1s - loss: 12499339444224.0000 - mae: 1712223.3750 - val_loss: 15015278018560.0000 - val_mae: 1873739.0000 - 707ms/epoch - 2ms/step\n",
      "Epoch 110/200\n",
      "341/341 - 1s - loss: 12506553647104.0000 - mae: 1722082.2500 - val_loss: 15014819790848.0000 - val_mae: 1843759.7500 - 675ms/epoch - 2ms/step\n",
      "Epoch 111/200\n",
      "341/341 - 1s - loss: 12485953323008.0000 - mae: 1708305.7500 - val_loss: 15026615222272.0000 - val_mae: 1890378.0000 - 688ms/epoch - 2ms/step\n",
      "Epoch 112/200\n",
      "341/341 - 1s - loss: 12497380704256.0000 - mae: 1727678.3750 - val_loss: 15006336811008.0000 - val_mae: 1857094.2500 - 692ms/epoch - 2ms/step\n",
      "Epoch 113/200\n",
      "341/341 - 1s - loss: 12491095539712.0000 - mae: 1722268.7500 - val_loss: 15016093810688.0000 - val_mae: 1831404.5000 - 667ms/epoch - 2ms/step\n",
      "Epoch 114/200\n",
      "341/341 - 1s - loss: 12485443715072.0000 - mae: 1705281.6250 - val_loss: 15017067937792.0000 - val_mae: 1888544.1250 - 656ms/epoch - 2ms/step\n",
      "Epoch 115/200\n",
      "341/341 - 1s - loss: 12486462930944.0000 - mae: 1726583.0000 - val_loss: 15008550354944.0000 - val_mae: 1850984.0000 - 690ms/epoch - 2ms/step\n",
      "Epoch 116/200\n",
      "341/341 - 1s - loss: 12483132653568.0000 - mae: 1712806.3750 - val_loss: 15004803792896.0000 - val_mae: 1858624.5000 - 678ms/epoch - 2ms/step\n",
      "Epoch 117/200\n",
      "341/341 - 1s - loss: 12482287501312.0000 - mae: 1717188.2500 - val_loss: 15002011435008.0000 - val_mae: 1865504.0000 - 675ms/epoch - 2ms/step\n",
      "Epoch 118/200\n",
      "341/341 - 1s - loss: 12477826859008.0000 - mae: 1721228.2500 - val_loss: 14999988731904.0000 - val_mae: 1862339.1250 - 669ms/epoch - 2ms/step\n",
      "Epoch 119/200\n",
      "341/341 - 1s - loss: 12474463027200.0000 - mae: 1718103.3750 - val_loss: 14997737439232.0000 - val_mae: 1860255.0000 - 683ms/epoch - 2ms/step\n",
      "Epoch 120/200\n",
      "341/341 - 1s - loss: 12466483363840.0000 - mae: 1714007.5000 - val_loss: 14992053108736.0000 - val_mae: 1848936.5000 - 705ms/epoch - 2ms/step\n",
      "Epoch 121/200\n",
      "341/341 - 1s - loss: 12470242508800.0000 - mae: 1717902.1250 - val_loss: 14985140895744.0000 - val_mae: 1868820.2500 - 677ms/epoch - 2ms/step\n",
      "Epoch 122/200\n",
      "341/341 - 1s - loss: 12462273331200.0000 - mae: 1713897.0000 - val_loss: 14995955908608.0000 - val_mae: 1872235.1250 - 677ms/epoch - 2ms/step\n",
      "Epoch 123/200\n",
      "341/341 - 1s - loss: 12465691688960.0000 - mae: 1720387.5000 - val_loss: 14992493510656.0000 - val_mae: 1868809.2500 - 675ms/epoch - 2ms/step\n",
      "Epoch 124/200\n",
      "341/341 - 1s - loss: 12457223389184.0000 - mae: 1721846.5000 - val_loss: 14994450153472.0000 - val_mae: 1829186.8750 - 662ms/epoch - 2ms/step\n",
      "Epoch 125/200\n",
      "341/341 - 1s - loss: 12468570030080.0000 - mae: 1706921.7500 - val_loss: 14983455834112.0000 - val_mae: 1851519.7500 - 663ms/epoch - 2ms/step\n",
      "Epoch 126/200\n",
      "341/341 - 1s - loss: 12460929056768.0000 - mae: 1721200.8750 - val_loss: 14984702590976.0000 - val_mae: 1834777.7500 - 663ms/epoch - 2ms/step\n",
      "Epoch 127/200\n",
      "341/341 - 1s - loss: 12454419496960.0000 - mae: 1715744.2500 - val_loss: 14983084638208.0000 - val_mae: 1840345.1250 - 657ms/epoch - 2ms/step\n",
      "Epoch 128/200\n",
      "341/341 - 1s - loss: 12453692833792.0000 - mae: 1701556.3750 - val_loss: 14981622923264.0000 - val_mae: 1875043.3750 - 702ms/epoch - 2ms/step\n",
      "Epoch 129/200\n",
      "341/341 - 1s - loss: 12457486581760.0000 - mae: 1721344.2500 - val_loss: 14970283622400.0000 - val_mae: 1850227.0000 - 692ms/epoch - 2ms/step\n",
      "Epoch 130/200\n",
      "341/341 - 1s - loss: 12453088854016.0000 - mae: 1713611.0000 - val_loss: 14976281477120.0000 - val_mae: 1868606.6250 - 692ms/epoch - 2ms/step\n",
      "Epoch 131/200\n",
      "341/341 - 1s - loss: 12449894891520.0000 - mae: 1713485.3750 - val_loss: 14972629286912.0000 - val_mae: 1851143.5000 - 668ms/epoch - 2ms/step\n",
      "Epoch 132/200\n",
      "341/341 - 1s - loss: 12449461829632.0000 - mae: 1718684.8750 - val_loss: 14966896721920.0000 - val_mae: 1854277.7500 - 666ms/epoch - 2ms/step\n",
      "Epoch 133/200\n",
      "341/341 - 1s - loss: 12444326952960.0000 - mae: 1713730.3750 - val_loss: 14968462245888.0000 - val_mae: 1856511.3750 - 671ms/epoch - 2ms/step\n",
      "Epoch 134/200\n",
      "341/341 - 1s - loss: 12447679250432.0000 - mae: 1712331.7500 - val_loss: 14962882772992.0000 - val_mae: 1855253.6250 - 673ms/epoch - 2ms/step\n",
      "Epoch 135/200\n",
      "341/341 - 1s - loss: 12452982947840.0000 - mae: 1719722.8750 - val_loss: 14963793985536.0000 - val_mae: 1835765.5000 - 655ms/epoch - 2ms/step\n",
      "Epoch 136/200\n",
      "341/341 - 1s - loss: 12429252624384.0000 - mae: 1693506.1250 - val_loss: 14979081175040.0000 - val_mae: 1892679.6250 - 639ms/epoch - 2ms/step\n",
      "Epoch 137/200\n",
      "341/341 - 1s - loss: 12439006478336.0000 - mae: 1722841.7500 - val_loss: 14958144258048.0000 - val_mae: 1848519.3750 - 648ms/epoch - 2ms/step\n",
      "Epoch 138/200\n",
      "341/341 - 1s - loss: 12431730409472.0000 - mae: 1721216.0000 - val_loss: 14979390504960.0000 - val_mae: 1820023.8750 - 664ms/epoch - 2ms/step\n",
      "Epoch 139/200\n",
      "341/341 - 1s - loss: 12425788129280.0000 - mae: 1696206.3750 - val_loss: 14961621336064.0000 - val_mae: 1873040.1250 - 658ms/epoch - 2ms/step\n",
      "Epoch 140/200\n",
      "341/341 - 1s - loss: 12427132403712.0000 - mae: 1711647.5000 - val_loss: 14965237874688.0000 - val_mae: 1876703.2500 - 649ms/epoch - 2ms/step\n",
      "Epoch 141/200\n",
      "341/341 - 1s - loss: 12432465461248.0000 - mae: 1722964.0000 - val_loss: 14960755212288.0000 - val_mae: 1838300.1250 - 656ms/epoch - 2ms/step\n",
      "Epoch 142/200\n",
      "341/341 - 1s - loss: 12429247381504.0000 - mae: 1712347.0000 - val_loss: 14952261746688.0000 - val_mae: 1836883.5000 - 677ms/epoch - 2ms/step\n",
      "Epoch 143/200\n",
      "341/341 - 1s - loss: 12428053053440.0000 - mae: 1708691.7500 - val_loss: 14951356825600.0000 - val_mae: 1848978.6250 - 653ms/epoch - 2ms/step\n",
      "Epoch 144/200\n",
      "341/341 - 1s - loss: 12421393547264.0000 - mae: 1710846.1250 - val_loss: 14953341779968.0000 - val_mae: 1859218.7500 - 679ms/epoch - 2ms/step\n",
      "Epoch 145/200\n",
      "341/341 - 1s - loss: 12426817830912.0000 - mae: 1711761.2500 - val_loss: 14951100973056.0000 - val_mae: 1853978.8750 - 649ms/epoch - 2ms/step\n",
      "Epoch 146/200\n",
      "341/341 - 1s - loss: 12425403301888.0000 - mae: 1714710.8750 - val_loss: 14948831854592.0000 - val_mae: 1845746.8750 - 671ms/epoch - 2ms/step\n",
      "Epoch 147/200\n",
      "341/341 - 1s - loss: 12414163615744.0000 - mae: 1718269.7500 - val_loss: 14951553957888.0000 - val_mae: 1831398.5000 - 654ms/epoch - 2ms/step\n",
      "Epoch 148/200\n",
      "341/341 - 1s - loss: 12418842361856.0000 - mae: 1698695.6250 - val_loss: 14943434833920.0000 - val_mae: 1863144.1250 - 662ms/epoch - 2ms/step\n",
      "Epoch 149/200\n",
      "341/341 - 1s - loss: 12421631574016.0000 - mae: 1709970.6250 - val_loss: 14954458513408.0000 - val_mae: 1871874.6250 - 655ms/epoch - 2ms/step\n",
      "Epoch 150/200\n",
      "341/341 - 1s - loss: 12419592093696.0000 - mae: 1714232.2500 - val_loss: 14942977654784.0000 - val_mae: 1860216.1250 - 659ms/epoch - 2ms/step\n",
      "Epoch 151/200\n",
      "341/341 - 1s - loss: 12412690366464.0000 - mae: 1715240.0000 - val_loss: 14942771085312.0000 - val_mae: 1845240.1250 - 685ms/epoch - 2ms/step\n",
      "Epoch 152/200\n",
      "341/341 - 1s - loss: 12409479626752.0000 - mae: 1717884.8750 - val_loss: 14945607483392.0000 - val_mae: 1832664.5000 - 672ms/epoch - 2ms/step\n",
      "Epoch 153/200\n",
      "341/341 - 1s - loss: 12407448535040.0000 - mae: 1693467.3750 - val_loss: 14938659618816.0000 - val_mae: 1862872.0000 - 655ms/epoch - 2ms/step\n",
      "Epoch 154/200\n",
      "341/341 - 1s - loss: 12402318901248.0000 - mae: 1722898.7500 - val_loss: 14946836414464.0000 - val_mae: 1831182.8750 - 668ms/epoch - 2ms/step\n",
      "Epoch 155/200\n",
      "341/341 - 1s - loss: 12402155323392.0000 - mae: 1697583.6250 - val_loss: 14944160448512.0000 - val_mae: 1871345.0000 - 661ms/epoch - 2ms/step\n",
      "Epoch 156/200\n",
      "341/341 - 1s - loss: 12408364990464.0000 - mae: 1714071.5000 - val_loss: 14933592899584.0000 - val_mae: 1863937.3750 - 645ms/epoch - 2ms/step\n",
      "Epoch 157/200\n",
      "341/341 - 1s - loss: 12405891399680.0000 - mae: 1710567.5000 - val_loss: 14933876015104.0000 - val_mae: 1848769.0000 - 642ms/epoch - 2ms/step\n",
      "Epoch 158/200\n",
      "341/341 - 1s - loss: 12400359112704.0000 - mae: 1714220.5000 - val_loss: 14947184541696.0000 - val_mae: 1825181.1250 - 700ms/epoch - 2ms/step\n",
      "Epoch 159/200\n",
      "341/341 - 1s - loss: 12402781323264.0000 - mae: 1699908.7500 - val_loss: 14925530398720.0000 - val_mae: 1856972.1250 - 648ms/epoch - 2ms/step\n",
      "Epoch 160/200\n",
      "341/341 - 1s - loss: 12399677538304.0000 - mae: 1706428.1250 - val_loss: 14937424396288.0000 - val_mae: 1877607.6250 - 650ms/epoch - 2ms/step\n",
      "Epoch 161/200\n",
      "341/341 - 1s - loss: 12398536687616.0000 - mae: 1710380.1250 - val_loss: 14931764183040.0000 - val_mae: 1861374.5000 - 676ms/epoch - 2ms/step\n",
      "Epoch 162/200\n",
      "341/341 - 1s - loss: 12402209849344.0000 - mae: 1716219.5000 - val_loss: 14932770816000.0000 - val_mae: 1841508.3750 - 657ms/epoch - 2ms/step\n",
      "Epoch 163/200\n",
      "341/341 - 1s - loss: 12399900884992.0000 - mae: 1709342.0000 - val_loss: 14927553101824.0000 - val_mae: 1853506.3750 - 654ms/epoch - 2ms/step\n",
      "Epoch 164/200\n",
      "341/341 - 1s - loss: 12391012106240.0000 - mae: 1711530.3750 - val_loss: 14921834168320.0000 - val_mae: 1833556.0000 - 650ms/epoch - 2ms/step\n",
      "Epoch 165/200\n",
      "341/341 - 1s - loss: 12391354990592.0000 - mae: 1714737.2500 - val_loss: 14931463241728.0000 - val_mae: 1850494.5000 - 646ms/epoch - 2ms/step\n",
      "Epoch 166/200\n",
      "341/341 - 1s - loss: 12388691607552.0000 - mae: 1704590.8750 - val_loss: 14925412958208.0000 - val_mae: 1846657.8750 - 645ms/epoch - 2ms/step\n",
      "Epoch 167/200\n",
      "341/341 - 1s - loss: 12386987671552.0000 - mae: 1704488.6250 - val_loss: 14923383963648.0000 - val_mae: 1862534.7500 - 685ms/epoch - 2ms/step\n",
      "Epoch 168/200\n",
      "341/341 - 1s - loss: 12385475624960.0000 - mae: 1707396.0000 - val_loss: 14925506281472.0000 - val_mae: 1854833.3750 - 686ms/epoch - 2ms/step\n",
      "Epoch 169/200\n",
      "341/341 - 1s - loss: 12390722699264.0000 - mae: 1711858.7500 - val_loss: 14925139279872.0000 - val_mae: 1848203.2500 - 653ms/epoch - 2ms/step\n",
      "Epoch 170/200\n",
      "341/341 - 1s - loss: 12386259959808.0000 - mae: 1705671.7500 - val_loss: 14918475579392.0000 - val_mae: 1847861.2500 - 650ms/epoch - 2ms/step\n",
      "Epoch 171/200\n",
      "341/341 - 1s - loss: 12371579895808.0000 - mae: 1708065.8750 - val_loss: 14938331414528.0000 - val_mae: 1830217.8750 - 651ms/epoch - 2ms/step\n",
      "Epoch 172/200\n",
      "341/341 - 1s - loss: 12382281662464.0000 - mae: 1708773.5000 - val_loss: 14927722971136.0000 - val_mae: 1842860.5000 - 645ms/epoch - 2ms/step\n",
      "Epoch 173/200\n",
      "341/341 - 1s - loss: 12383435096064.0000 - mae: 1710606.6250 - val_loss: 14921493381120.0000 - val_mae: 1834973.3750 - 681ms/epoch - 2ms/step\n",
      "Epoch 174/200\n",
      "341/341 - 1s - loss: 12380533686272.0000 - mae: 1714666.1250 - val_loss: 14933320269824.0000 - val_mae: 1821884.3750 - 710ms/epoch - 2ms/step\n",
      "Epoch 175/200\n",
      "341/341 - 1s - loss: 12381357867008.0000 - mae: 1705987.0000 - val_loss: 14938850459648.0000 - val_mae: 1818601.8750 - 668ms/epoch - 2ms/step\n",
      "Epoch 176/200\n",
      "341/341 - 1s - loss: 12378575994880.0000 - mae: 1698392.8750 - val_loss: 14914540273664.0000 - val_mae: 1847209.7500 - 660ms/epoch - 2ms/step\n",
      "Epoch 177/200\n",
      "341/341 - 1s - loss: 12374459285504.0000 - mae: 1710066.8750 - val_loss: 14914761523200.0000 - val_mae: 1843613.8750 - 678ms/epoch - 2ms/step\n",
      "Epoch 178/200\n",
      "341/341 - 1s - loss: 12375055925248.0000 - mae: 1705913.1250 - val_loss: 14914690220032.0000 - val_mae: 1834487.8750 - 650ms/epoch - 2ms/step\n",
      "Epoch 179/200\n",
      "341/341 - 1s - loss: 12367419146240.0000 - mae: 1704997.3750 - val_loss: 14912908689408.0000 - val_mae: 1840110.3750 - 649ms/epoch - 2ms/step\n",
      "Epoch 180/200\n",
      "341/341 - 1s - loss: 12377403686912.0000 - mae: 1707237.5000 - val_loss: 14913078558720.0000 - val_mae: 1843389.8750 - 648ms/epoch - 2ms/step\n",
      "Epoch 181/200\n",
      "341/341 - 1s - loss: 12372377862144.0000 - mae: 1705850.0000 - val_loss: 14908125085696.0000 - val_mae: 1842964.5000 - 654ms/epoch - 2ms/step\n",
      "Epoch 182/200\n",
      "341/341 - 1s - loss: 12362016882688.0000 - mae: 1705576.3750 - val_loss: 14913243185152.0000 - val_mae: 1837190.3750 - 646ms/epoch - 2ms/step\n",
      "Epoch 183/200\n",
      "341/341 - 1s - loss: 12365742473216.0000 - mae: 1702895.0000 - val_loss: 14911327436800.0000 - val_mae: 1842738.1250 - 643ms/epoch - 2ms/step\n",
      "Epoch 184/200\n",
      "341/341 - 1s - loss: 12365586235392.0000 - mae: 1712844.0000 - val_loss: 14914336849920.0000 - val_mae: 1843765.3750 - 653ms/epoch - 2ms/step\n",
      "Epoch 185/200\n",
      "341/341 - 1s - loss: 12364072091648.0000 - mae: 1701271.1250 - val_loss: 14914265546752.0000 - val_mae: 1836053.2500 - 651ms/epoch - 2ms/step\n",
      "Epoch 186/200\n",
      "341/341 - 1s - loss: 12367181119488.0000 - mae: 1700146.8750 - val_loss: 14908423929856.0000 - val_mae: 1853978.1250 - 649ms/epoch - 2ms/step\n",
      "Epoch 187/200\n",
      "341/341 - 1s - loss: 12354277343232.0000 - mae: 1717271.3750 - val_loss: 14926187855872.0000 - val_mae: 1814749.3750 - 658ms/epoch - 2ms/step\n",
      "Epoch 188/200\n",
      "341/341 - 1s - loss: 12355009249280.0000 - mae: 1692263.7500 - val_loss: 14912914980864.0000 - val_mae: 1856317.2500 - 651ms/epoch - 2ms/step\n",
      "Epoch 189/200\n",
      "341/341 - 1s - loss: 12362646028288.0000 - mae: 1705297.3750 - val_loss: 14906848968704.0000 - val_mae: 1846644.7500 - 666ms/epoch - 2ms/step\n",
      "Epoch 190/200\n",
      "341/341 - 1s - loss: 12353592623104.0000 - mae: 1702588.7500 - val_loss: 14905442828288.0000 - val_mae: 1859326.8750 - 714ms/epoch - 2ms/step\n",
      "Epoch 191/200\n",
      "341/341 - 1s - loss: 12353595768832.0000 - mae: 1713277.1250 - val_loss: 14906995769344.0000 - val_mae: 1830930.6250 - 745ms/epoch - 2ms/step\n",
      "Epoch 192/200\n",
      "341/341 - 1s - loss: 12357600280576.0000 - mae: 1703045.6250 - val_loss: 14912013205504.0000 - val_mae: 1820959.6250 - 672ms/epoch - 2ms/step\n",
      "Epoch 193/200\n",
      "341/341 - 1s - loss: 12354677899264.0000 - mae: 1694031.2500 - val_loss: 14907223310336.0000 - val_mae: 1865704.3750 - 663ms/epoch - 2ms/step\n",
      "Epoch 194/200\n",
      "341/341 - 1s - loss: 12348325625856.0000 - mae: 1709002.8750 - val_loss: 14908106211328.0000 - val_mae: 1828932.6250 - 659ms/epoch - 2ms/step\n",
      "Epoch 195/200\n",
      "341/341 - 1s - loss: 12349804118016.0000 - mae: 1701367.0000 - val_loss: 14901896544256.0000 - val_mae: 1851869.7500 - 661ms/epoch - 2ms/step\n",
      "Epoch 196/200\n",
      "341/341 - 1s - loss: 12349223206912.0000 - mae: 1705610.0000 - val_loss: 14900669710336.0000 - val_mae: 1848703.3750 - 639ms/epoch - 2ms/step\n",
      "Epoch 197/200\n",
      "341/341 - 1s - loss: 12338143952896.0000 - mae: 1703353.8750 - val_loss: 14906776616960.0000 - val_mae: 1822263.2500 - 686ms/epoch - 2ms/step\n",
      "Epoch 198/200\n",
      "341/341 - 1s - loss: 12333297434624.0000 - mae: 1695955.8750 - val_loss: 14911883182080.0000 - val_mae: 1875956.1250 - 693ms/epoch - 2ms/step\n",
      "Epoch 199/200\n",
      "341/341 - 1s - loss: 12350278074368.0000 - mae: 1709544.0000 - val_loss: 14901101723648.0000 - val_mae: 1856841.2500 - 665ms/epoch - 2ms/step\n",
      "Epoch 200/200\n",
      "341/341 - 1s - loss: 12351103303680.0000 - mae: 1701590.2500 - val_loss: 14896451289088.0000 - val_mae: 1852677.8750 - 655ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb1592e94b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.fit(\n",
    "    X_train_scaled, y_train, \n",
    "    epochs=200, verbose=2, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13973050097664.0, 1792704.25]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.evaluate(X_test_scaled, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_SCORE: 0.527097, MAPE: 29.128152, MSE: 13973048813926.271484, RMSE: 3738054.148073, MAE: 1792704.042437\n"
     ]
    }
   ],
   "source": [
    "pred_mlp = model_mlp.predict(X_test_scaled, verbose=0)\n",
    "_ = regression_evaluation(y_test, pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t213p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
