{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기본적인 DL 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from freeman.plt_setting import plt_settings\n",
    "from freeman.evaluation import regression_evaluation, f_importances, plot_actual_pred\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils.data_manager import read_data\n",
    "\n",
    "# 한글처리 지원\n",
    "plt_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = read_data('2nd pp counts-base-on-cons-1st')\n",
    "\n",
    "feature_columns = [\n",
    "    'year', 'month', 'day', 'dayofweek', 'dayofyear',\n",
    "    '사번코드숫자', '사번숫자', '사업소코드', '계약전력', \n",
    "    'line_cnts', 'pole_cnts', 'sl_cnts'\n",
    "]\n",
    "target_column = '총공사비'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = df_data[feature_columns + [target_column]]\n",
    "df_y = df_X.pop(target_column)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(df_X, df_y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "X_train_scaled = standard_scaler.fit_transform(X_train)\n",
    "X_test_scaled = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DL Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-16 17:40:20.212311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-16 17:40:20.254413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-16 17:40:20.254705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-16 17:40:20.256200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-16 17:40:20.256467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-16 17:40:20.256745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-16 17:40:21.498702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-16 17:40:21.499390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-16 17:40:21.499975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-16 17:40:21.500563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6823 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               1664      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12033 (47.00 KB)\n",
      "Trainable params: 12033 (47.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1:])),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model_mlp.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 - 1s - loss: 13769787834368.0000 - mae: 1811879.1250 - val_loss: 13977730940928.0000 - val_mae: 1899037.7500 - 715ms/epoch - 2ms/step\n",
      "Epoch 2/200\n",
      "341/341 - 1s - loss: 13771199217664.0000 - mae: 1804267.7500 - val_loss: 13974881959936.0000 - val_mae: 1883938.2500 - 679ms/epoch - 2ms/step\n",
      "Epoch 3/200\n",
      "341/341 - 1s - loss: 13768436219904.0000 - mae: 1798858.8750 - val_loss: 13971738329088.0000 - val_mae: 1907028.6250 - 691ms/epoch - 2ms/step\n",
      "Epoch 4/200\n",
      "341/341 - 1s - loss: 13765906006016.0000 - mae: 1802323.6250 - val_loss: 13968059924480.0000 - val_mae: 1902632.2500 - 659ms/epoch - 2ms/step\n",
      "Epoch 5/200\n",
      "341/341 - 1s - loss: 13767946534912.0000 - mae: 1800536.2500 - val_loss: 13979713798144.0000 - val_mae: 1913485.3750 - 669ms/epoch - 2ms/step\n",
      "Epoch 6/200\n",
      "341/341 - 1s - loss: 13762949021696.0000 - mae: 1806641.7500 - val_loss: 13974738305024.0000 - val_mae: 1881413.3750 - 669ms/epoch - 2ms/step\n",
      "Epoch 7/200\n",
      "341/341 - 1s - loss: 13760493256704.0000 - mae: 1802349.3750 - val_loss: 13975338090496.0000 - val_mae: 1893998.3750 - 660ms/epoch - 2ms/step\n",
      "Epoch 8/200\n",
      "341/341 - 1s - loss: 13762292613120.0000 - mae: 1801369.1250 - val_loss: 13974327263232.0000 - val_mae: 1884448.8750 - 653ms/epoch - 2ms/step\n",
      "Epoch 9/200\n",
      "341/341 - 1s - loss: 13754626473984.0000 - mae: 1800694.7500 - val_loss: 13972088553472.0000 - val_mae: 1902393.8750 - 658ms/epoch - 2ms/step\n",
      "Epoch 10/200\n",
      "341/341 - 1s - loss: 13760001474560.0000 - mae: 1809777.3750 - val_loss: 13966225965056.0000 - val_mae: 1883249.0000 - 691ms/epoch - 2ms/step\n",
      "Epoch 11/200\n",
      "341/341 - 1s - loss: 13750768762880.0000 - mae: 1784083.2500 - val_loss: 13968059924480.0000 - val_mae: 1907529.1250 - 661ms/epoch - 2ms/step\n",
      "Epoch 12/200\n",
      "341/341 - 1s - loss: 13755067924480.0000 - mae: 1804792.3750 - val_loss: 13972438777856.0000 - val_mae: 1913765.0000 - 688ms/epoch - 2ms/step\n",
      "Epoch 13/200\n",
      "341/341 - 1s - loss: 13750467821568.0000 - mae: 1809381.7500 - val_loss: 13959651393536.0000 - val_mae: 1881390.6250 - 671ms/epoch - 2ms/step\n",
      "Epoch 14/200\n",
      "341/341 - 1s - loss: 13750344089600.0000 - mae: 1801910.0000 - val_loss: 13971512885248.0000 - val_mae: 1863434.8750 - 678ms/epoch - 2ms/step\n",
      "Epoch 15/200\n",
      "341/341 - 1s - loss: 13755287076864.0000 - mae: 1793260.2500 - val_loss: 13958080626688.0000 - val_mae: 1893412.8750 - 671ms/epoch - 2ms/step\n",
      "Epoch 16/200\n",
      "341/341 - 1s - loss: 13754369572864.0000 - mae: 1808165.7500 - val_loss: 13968677535744.0000 - val_mae: 1878776.8750 - 666ms/epoch - 2ms/step\n",
      "Epoch 17/200\n",
      "341/341 - 1s - loss: 13747204653056.0000 - mae: 1796273.5000 - val_loss: 13961329115136.0000 - val_mae: 1887290.5000 - 657ms/epoch - 2ms/step\n",
      "Epoch 18/200\n",
      "341/341 - 1s - loss: 13747401785344.0000 - mae: 1797668.5000 - val_loss: 13966993522688.0000 - val_mae: 1900986.8750 - 655ms/epoch - 2ms/step\n",
      "Epoch 19/200\n",
      "341/341 - 1s - loss: 13747470991360.0000 - mae: 1801044.8750 - val_loss: 13965565362176.0000 - val_mae: 1900287.6250 - 658ms/epoch - 2ms/step\n",
      "Epoch 20/200\n",
      "341/341 - 1s - loss: 13747709018112.0000 - mae: 1808652.6250 - val_loss: 13957387517952.0000 - val_mae: 1873849.3750 - 651ms/epoch - 2ms/step\n",
      "Epoch 21/200\n",
      "341/341 - 1s - loss: 13746160271360.0000 - mae: 1794243.5000 - val_loss: 13957226037248.0000 - val_mae: 1887878.6250 - 666ms/epoch - 2ms/step\n",
      "Epoch 22/200\n",
      "341/341 - 1s - loss: 13741133398016.0000 - mae: 1801924.5000 - val_loss: 13965808631808.0000 - val_mae: 1903985.7500 - 668ms/epoch - 2ms/step\n",
      "Epoch 23/200\n",
      "341/341 - 1s - loss: 13740106842112.0000 - mae: 1809207.8750 - val_loss: 13959192117248.0000 - val_mae: 1878353.2500 - 657ms/epoch - 2ms/step\n",
      "Epoch 24/200\n",
      "341/341 - 1s - loss: 13738535026688.0000 - mae: 1795316.8750 - val_loss: 13954176778240.0000 - val_mae: 1891577.6250 - 665ms/epoch - 2ms/step\n",
      "Epoch 25/200\n",
      "341/341 - 1s - loss: 13741851672576.0000 - mae: 1798378.2500 - val_loss: 13958848184320.0000 - val_mae: 1878219.3750 - 692ms/epoch - 2ms/step\n",
      "Epoch 26/200\n",
      "341/341 - 1s - loss: 13743925755904.0000 - mae: 1807222.5000 - val_loss: 13965426950144.0000 - val_mae: 1874269.7500 - 675ms/epoch - 2ms/step\n",
      "Epoch 27/200\n",
      "341/341 - 1s - loss: 13737222209536.0000 - mae: 1790167.2500 - val_loss: 13965834846208.0000 - val_mae: 1887455.2500 - 660ms/epoch - 2ms/step\n",
      "Epoch 28/200\n",
      "341/341 - 1s - loss: 13741239304192.0000 - mae: 1800433.3750 - val_loss: 13958850281472.0000 - val_mae: 1891038.0000 - 662ms/epoch - 2ms/step\n",
      "Epoch 29/200\n",
      "341/341 - 1s - loss: 13735503593472.0000 - mae: 1801797.6250 - val_loss: 13954795438080.0000 - val_mae: 1888235.1250 - 681ms/epoch - 2ms/step\n",
      "Epoch 30/200\n",
      "341/341 - 1s - loss: 13737525248000.0000 - mae: 1794677.6250 - val_loss: 13948721037312.0000 - val_mae: 1884210.3750 - 687ms/epoch - 2ms/step\n",
      "Epoch 31/200\n",
      "341/341 - 1s - loss: 13736268005376.0000 - mae: 1799771.2500 - val_loss: 13956603183104.0000 - val_mae: 1906796.6250 - 650ms/epoch - 2ms/step\n",
      "Epoch 32/200\n",
      "341/341 - 1s - loss: 13732882153472.0000 - mae: 1802070.2500 - val_loss: 13958356402176.0000 - val_mae: 1886794.6250 - 650ms/epoch - 2ms/step\n",
      "Epoch 33/200\n",
      "341/341 - 1s - loss: 13731309289472.0000 - mae: 1797283.2500 - val_loss: 13949181362176.0000 - val_mae: 1882782.2500 - 668ms/epoch - 2ms/step\n",
      "Epoch 34/200\n",
      "341/341 - 1s - loss: 13729805631488.0000 - mae: 1793282.3750 - val_loss: 13956693360640.0000 - val_mae: 1911740.5000 - 643ms/epoch - 2ms/step\n",
      "Epoch 35/200\n",
      "341/341 - 1s - loss: 13734254739456.0000 - mae: 1804988.1250 - val_loss: 13949822042112.0000 - val_mae: 1891023.0000 - 648ms/epoch - 2ms/step\n",
      "Epoch 36/200\n",
      "341/341 - 1s - loss: 13725432020992.0000 - mae: 1807442.1250 - val_loss: 13952385810432.0000 - val_mae: 1867584.2500 - 649ms/epoch - 2ms/step\n",
      "Epoch 37/200\n",
      "341/341 - 1s - loss: 13731157245952.0000 - mae: 1794441.3750 - val_loss: 13946731888640.0000 - val_mae: 1881965.7500 - 647ms/epoch - 2ms/step\n",
      "Epoch 38/200\n",
      "341/341 - 1s - loss: 13718054240256.0000 - mae: 1785694.8750 - val_loss: 13957561581568.0000 - val_mae: 1916490.6250 - 643ms/epoch - 2ms/step\n",
      "Epoch 39/200\n",
      "341/341 - 1s - loss: 13729033879552.0000 - mae: 1804474.3750 - val_loss: 13958788415488.0000 - val_mae: 1886007.2500 - 652ms/epoch - 2ms/step\n",
      "Epoch 40/200\n",
      "341/341 - 1s - loss: 13714510053376.0000 - mae: 1788199.1250 - val_loss: 13951694798848.0000 - val_mae: 1914367.7500 - 651ms/epoch - 2ms/step\n",
      "Epoch 41/200\n",
      "341/341 - 1s - loss: 13722105937920.0000 - mae: 1815460.1250 - val_loss: 13945286950912.0000 - val_mae: 1865911.1250 - 668ms/epoch - 2ms/step\n",
      "Epoch 42/200\n",
      "341/341 - 1s - loss: 13720455479296.0000 - mae: 1790692.0000 - val_loss: 13936192651264.0000 - val_mae: 1891511.1250 - 660ms/epoch - 2ms/step\n",
      "Epoch 43/200\n",
      "341/341 - 1s - loss: 13715376177152.0000 - mae: 1813034.8750 - val_loss: 13947286585344.0000 - val_mae: 1856283.2500 - 666ms/epoch - 2ms/step\n",
      "Epoch 44/200\n",
      "341/341 - 1s - loss: 13720010883072.0000 - mae: 1782679.7500 - val_loss: 13938794168320.0000 - val_mae: 1889429.7500 - 646ms/epoch - 2ms/step\n",
      "Epoch 45/200\n",
      "341/341 - 1s - loss: 13714975621120.0000 - mae: 1790043.0000 - val_loss: 13948210380800.0000 - val_mae: 1910397.5000 - 645ms/epoch - 2ms/step\n",
      "Epoch 46/200\n",
      "341/341 - 1s - loss: 13722448822272.0000 - mae: 1800529.8750 - val_loss: 13945878347776.0000 - val_mae: 1911450.3750 - 681ms/epoch - 2ms/step\n",
      "Epoch 47/200\n",
      "341/341 - 1s - loss: 13717987131392.0000 - mae: 1794422.3750 - val_loss: 13944676679680.0000 - val_mae: 1904573.6250 - 662ms/epoch - 2ms/step\n",
      "Epoch 48/200\n",
      "341/341 - 1s - loss: 13715039584256.0000 - mae: 1810072.5000 - val_loss: 13936833331200.0000 - val_mae: 1891647.5000 - 650ms/epoch - 2ms/step\n",
      "Epoch 49/200\n",
      "341/341 - 1s - loss: 13713020026880.0000 - mae: 1788194.1250 - val_loss: 13938005639168.0000 - val_mae: 1913170.7500 - 634ms/epoch - 2ms/step\n",
      "Epoch 50/200\n",
      "341/341 - 1s - loss: 13713362911232.0000 - mae: 1803651.7500 - val_loss: 13937454088192.0000 - val_mae: 1891396.3750 - 661ms/epoch - 2ms/step\n",
      "Epoch 51/200\n",
      "341/341 - 1s - loss: 13710287437824.0000 - mae: 1801955.8750 - val_loss: 13943013638144.0000 - val_mae: 1881425.6250 - 647ms/epoch - 2ms/step\n",
      "Epoch 52/200\n",
      "341/341 - 1s - loss: 13710724694016.0000 - mae: 1805017.6250 - val_loss: 13941348499456.0000 - val_mae: 1864905.6250 - 650ms/epoch - 2ms/step\n",
      "Epoch 53/200\n",
      "341/341 - 1s - loss: 13710642905088.0000 - mae: 1789368.0000 - val_loss: 13934864105472.0000 - val_mae: 1866413.6250 - 654ms/epoch - 2ms/step\n",
      "Epoch 54/200\n",
      "341/341 - 1s - loss: 13707567431680.0000 - mae: 1793114.0000 - val_loss: 13930600595456.0000 - val_mae: 1878013.7500 - 673ms/epoch - 2ms/step\n",
      "Epoch 55/200\n",
      "341/341 - 1s - loss: 13704932360192.0000 - mae: 1789873.7500 - val_loss: 13930683432960.0000 - val_mae: 1893840.8750 - 649ms/epoch - 2ms/step\n",
      "Epoch 56/200\n",
      "341/341 - 1s - loss: 13707652366336.0000 - mae: 1799011.2500 - val_loss: 13928230813696.0000 - val_mae: 1888318.0000 - 643ms/epoch - 2ms/step\n",
      "Epoch 57/200\n",
      "341/341 - 1s - loss: 13705242738688.0000 - mae: 1797791.1250 - val_loss: 13926502760448.0000 - val_mae: 1893014.0000 - 661ms/epoch - 2ms/step\n",
      "Epoch 58/200\n",
      "341/341 - 1s - loss: 13697522073600.0000 - mae: 1807288.2500 - val_loss: 13930616324096.0000 - val_mae: 1860848.2500 - 678ms/epoch - 2ms/step\n",
      "Epoch 59/200\n",
      "341/341 - 1s - loss: 13701951258624.0000 - mae: 1791009.1250 - val_loss: 13927943503872.0000 - val_mae: 1863258.2500 - 682ms/epoch - 2ms/step\n",
      "Epoch 60/200\n",
      "341/341 - 1s - loss: 13702758662144.0000 - mae: 1786565.1250 - val_loss: 13931950112768.0000 - val_mae: 1894966.1250 - 674ms/epoch - 2ms/step\n",
      "Epoch 61/200\n",
      "341/341 - 1s - loss: 13698246639616.0000 - mae: 1796697.2500 - val_loss: 13923328720896.0000 - val_mae: 1894751.1250 - 672ms/epoch - 2ms/step\n",
      "Epoch 62/200\n",
      "341/341 - 1s - loss: 13695399755776.0000 - mae: 1800840.2500 - val_loss: 13927420264448.0000 - val_mae: 1878956.8750 - 648ms/epoch - 2ms/step\n",
      "Epoch 63/200\n",
      "341/341 - 1s - loss: 13696372834304.0000 - mae: 1791732.1250 - val_loss: 13920897073152.0000 - val_mae: 1882741.2500 - 649ms/epoch - 2ms/step\n",
      "Epoch 64/200\n",
      "341/341 - 1s - loss: 13691829354496.0000 - mae: 1800386.6250 - val_loss: 13928097644544.0000 - val_mae: 1858195.8750 - 650ms/epoch - 2ms/step\n",
      "Epoch 65/200\n",
      "341/341 - 1s - loss: 13696901316608.0000 - mae: 1793788.1250 - val_loss: 13924371005440.0000 - val_mae: 1867148.6250 - 653ms/epoch - 2ms/step\n",
      "Epoch 66/200\n",
      "341/341 - 1s - loss: 13697011417088.0000 - mae: 1787249.3750 - val_loss: 13927402438656.0000 - val_mae: 1898563.1250 - 654ms/epoch - 2ms/step\n",
      "Epoch 67/200\n",
      "341/341 - 1s - loss: 13696535363584.0000 - mae: 1802661.3750 - val_loss: 13925289558016.0000 - val_mae: 1872692.2500 - 651ms/epoch - 2ms/step\n",
      "Epoch 68/200\n",
      "341/341 - 1s - loss: 13695982764032.0000 - mae: 1789966.5000 - val_loss: 13921417166848.0000 - val_mae: 1884078.0000 - 649ms/epoch - 2ms/step\n",
      "Epoch 69/200\n",
      "341/341 - 1s - loss: 13696708378624.0000 - mae: 1795371.3750 - val_loss: 13922602057728.0000 - val_mae: 1875088.0000 - 651ms/epoch - 2ms/step\n",
      "Epoch 70/200\n",
      "341/341 - 1s - loss: 13690516537344.0000 - mae: 1787872.7500 - val_loss: 13926985105408.0000 - val_mae: 1916693.3750 - 652ms/epoch - 2ms/step\n",
      "Epoch 71/200\n",
      "341/341 - 1s - loss: 13695033802752.0000 - mae: 1802898.2500 - val_loss: 13925567430656.0000 - val_mae: 1901828.3750 - 642ms/epoch - 2ms/step\n",
      "Epoch 72/200\n",
      "341/341 - 1s - loss: 13686151315456.0000 - mae: 1796236.6250 - val_loss: 13914577305600.0000 - val_mae: 1887816.7500 - 638ms/epoch - 2ms/step\n",
      "Epoch 73/200\n",
      "341/341 - 1s - loss: 13678538653696.0000 - mae: 1792076.8750 - val_loss: 13918248370176.0000 - val_mae: 1904814.3750 - 644ms/epoch - 2ms/step\n",
      "Epoch 74/200\n",
      "341/341 - 1s - loss: 13682040897536.0000 - mae: 1792760.5000 - val_loss: 13909943648256.0000 - val_mae: 1897471.2500 - 650ms/epoch - 2ms/step\n",
      "Epoch 75/200\n",
      "341/341 - 1s - loss: 13683217399808.0000 - mae: 1808717.1250 - val_loss: 13913089376256.0000 - val_mae: 1868705.2500 - 667ms/epoch - 2ms/step\n",
      "Epoch 76/200\n",
      "341/341 - 1s - loss: 13682324013056.0000 - mae: 1777072.3750 - val_loss: 13916203646976.0000 - val_mae: 1897360.3750 - 658ms/epoch - 2ms/step\n",
      "Epoch 77/200\n",
      "341/341 - 1s - loss: 13677928382464.0000 - mae: 1804370.7500 - val_loss: 13914899218432.0000 - val_mae: 1873059.7500 - 646ms/epoch - 2ms/step\n",
      "Epoch 78/200\n",
      "341/341 - 1s - loss: 13681824890880.0000 - mae: 1791557.6250 - val_loss: 13910756294656.0000 - val_mae: 1880016.5000 - 643ms/epoch - 2ms/step\n",
      "Epoch 79/200\n",
      "341/341 - 1s - loss: 13679800090624.0000 - mae: 1798003.8750 - val_loss: 13908189380608.0000 - val_mae: 1868581.7500 - 695ms/epoch - 2ms/step\n",
      "Epoch 80/200\n",
      "341/341 - 1s - loss: 13676986761216.0000 - mae: 1788116.1250 - val_loss: 13903819964416.0000 - val_mae: 1880252.1250 - 713ms/epoch - 2ms/step\n",
      "Epoch 81/200\n",
      "341/341 - 1s - loss: 13673361833984.0000 - mae: 1787844.3750 - val_loss: 13902001733632.0000 - val_mae: 1893765.5000 - 666ms/epoch - 2ms/step\n",
      "Epoch 82/200\n",
      "341/341 - 1s - loss: 13674447110144.0000 - mae: 1798019.7500 - val_loss: 13904969203712.0000 - val_mae: 1901410.5000 - 651ms/epoch - 2ms/step\n",
      "Epoch 83/200\n",
      "341/341 - 1s - loss: 13673623977984.0000 - mae: 1795996.1250 - val_loss: 13902353006592.0000 - val_mae: 1884423.8750 - 653ms/epoch - 2ms/step\n",
      "Epoch 84/200\n",
      "341/341 - 1s - loss: 13660792553472.0000 - mae: 1802093.5000 - val_loss: 13910331621376.0000 - val_mae: 1854239.7500 - 677ms/epoch - 2ms/step\n",
      "Epoch 85/200\n",
      "341/341 - 1s - loss: 13668102176768.0000 - mae: 1776638.2500 - val_loss: 13902542798848.0000 - val_mae: 1905756.5000 - 642ms/epoch - 2ms/step\n",
      "Epoch 86/200\n",
      "341/341 - 1s - loss: 13668985077760.0000 - mae: 1798641.1250 - val_loss: 13898263560192.0000 - val_mae: 1894960.2500 - 666ms/epoch - 2ms/step\n",
      "Epoch 87/200\n",
      "341/341 - 1s - loss: 13654944645120.0000 - mae: 1803297.7500 - val_loss: 13909067038720.0000 - val_mae: 1846311.2500 - 670ms/epoch - 2ms/step\n",
      "Epoch 88/200\n",
      "341/341 - 1s - loss: 13665430405120.0000 - mae: 1770298.5000 - val_loss: 13912701403136.0000 - val_mae: 1923431.7500 - 741ms/epoch - 2ms/step\n",
      "Epoch 89/200\n",
      "341/341 - 1s - loss: 13662577229824.0000 - mae: 1811901.2500 - val_loss: 13895339081728.0000 - val_mae: 1865403.6250 - 680ms/epoch - 2ms/step\n",
      "Epoch 90/200\n",
      "341/341 - 1s - loss: 13659298332672.0000 - mae: 1782385.1250 - val_loss: 13902850031616.0000 - val_mae: 1912146.5000 - 650ms/epoch - 2ms/step\n",
      "Epoch 91/200\n",
      "341/341 - 1s - loss: 13659292041216.0000 - mae: 1799964.1250 - val_loss: 13892613832704.0000 - val_mae: 1874590.8750 - 639ms/epoch - 2ms/step\n",
      "Epoch 92/200\n",
      "341/341 - 1s - loss: 13657811451904.0000 - mae: 1792022.7500 - val_loss: 13892559306752.0000 - val_mae: 1878107.5000 - 648ms/epoch - 2ms/step\n",
      "Epoch 93/200\n",
      "341/341 - 1s - loss: 13664656556032.0000 - mae: 1790244.7500 - val_loss: 13885835837440.0000 - val_mae: 1881876.1250 - 646ms/epoch - 2ms/step\n",
      "Epoch 94/200\n",
      "341/341 - 1s - loss: 13650288967680.0000 - mae: 1801725.0000 - val_loss: 13885963763712.0000 - val_mae: 1864764.5000 - 643ms/epoch - 2ms/step\n",
      "Epoch 95/200\n",
      "341/341 - 1s - loss: 13656100175872.0000 - mae: 1783573.8750 - val_loss: 13889679917056.0000 - val_mae: 1882451.1250 - 654ms/epoch - 2ms/step\n",
      "Epoch 96/200\n",
      "341/341 - 1s - loss: 13651745439744.0000 - mae: 1782909.2500 - val_loss: 13897863004160.0000 - val_mae: 1902695.6250 - 645ms/epoch - 2ms/step\n",
      "Epoch 97/200\n",
      "341/341 - 1s - loss: 13650220810240.0000 - mae: 1800785.5000 - val_loss: 13884968665088.0000 - val_mae: 1888187.2500 - 658ms/epoch - 2ms/step\n",
      "Epoch 98/200\n",
      "341/341 - 1s - loss: 13650555305984.0000 - mae: 1787527.6250 - val_loss: 13883299332096.0000 - val_mae: 1899013.5000 - 647ms/epoch - 2ms/step\n",
      "Epoch 99/200\n",
      "341/341 - 1s - loss: 13647171551232.0000 - mae: 1797592.3750 - val_loss: 13879204642816.0000 - val_mae: 1893517.0000 - 636ms/epoch - 2ms/step\n",
      "Epoch 100/200\n",
      "341/341 - 1s - loss: 13648643751936.0000 - mae: 1788609.0000 - val_loss: 13882780286976.0000 - val_mae: 1896808.0000 - 639ms/epoch - 2ms/step\n",
      "Epoch 101/200\n",
      "341/341 - 1s - loss: 13647774482432.0000 - mae: 1790991.5000 - val_loss: 13878491611136.0000 - val_mae: 1896655.1250 - 648ms/epoch - 2ms/step\n",
      "Epoch 102/200\n",
      "341/341 - 1s - loss: 13644161089536.0000 - mae: 1797078.7500 - val_loss: 13870333689856.0000 - val_mae: 1880664.8750 - 644ms/epoch - 2ms/step\n",
      "Epoch 103/200\n",
      "341/341 - 1s - loss: 13641651847168.0000 - mae: 1789004.2500 - val_loss: 13877171453952.0000 - val_mae: 1889980.1250 - 665ms/epoch - 2ms/step\n",
      "Epoch 104/200\n",
      "341/341 - 1s - loss: 13640569716736.0000 - mae: 1790904.2500 - val_loss: 13866080665600.0000 - val_mae: 1877449.2500 - 645ms/epoch - 2ms/step\n",
      "Epoch 105/200\n",
      "341/341 - 1s - loss: 13637930450944.0000 - mae: 1795668.2500 - val_loss: 13866446618624.0000 - val_mae: 1860266.8750 - 662ms/epoch - 2ms/step\n",
      "Epoch 106/200\n",
      "341/341 - 1s - loss: 13642227515392.0000 - mae: 1780544.1250 - val_loss: 13865006923776.0000 - val_mae: 1874759.2500 - 670ms/epoch - 2ms/step\n",
      "Epoch 107/200\n",
      "341/341 - 1s - loss: 13631892750336.0000 - mae: 1796764.3750 - val_loss: 13863777992704.0000 - val_mae: 1877011.8750 - 638ms/epoch - 2ms/step\n",
      "Epoch 108/200\n",
      "341/341 - 1s - loss: 13634231074816.0000 - mae: 1784678.5000 - val_loss: 13846280404992.0000 - val_mae: 1886736.8750 - 648ms/epoch - 2ms/step\n",
      "Epoch 109/200\n",
      "341/341 - 1s - loss: 13631439765504.0000 - mae: 1790649.7500 - val_loss: 13856459980800.0000 - val_mae: 1893972.2500 - 669ms/epoch - 2ms/step\n",
      "Epoch 110/200\n",
      "341/341 - 1s - loss: 13629311156224.0000 - mae: 1785824.6250 - val_loss: 13855330664448.0000 - val_mae: 1894942.5000 - 664ms/epoch - 2ms/step\n",
      "Epoch 111/200\n",
      "341/341 - 1s - loss: 13625497485312.0000 - mae: 1794403.8750 - val_loss: 13855711297536.0000 - val_mae: 1898584.3750 - 668ms/epoch - 2ms/step\n",
      "Epoch 112/200\n",
      "341/341 - 1s - loss: 13618868387840.0000 - mae: 1793333.3750 - val_loss: 13845571567616.0000 - val_mae: 1876558.0000 - 654ms/epoch - 2ms/step\n",
      "Epoch 113/200\n",
      "341/341 - 1s - loss: 13616425205760.0000 - mae: 1784859.3750 - val_loss: 13844948713472.0000 - val_mae: 1891411.8750 - 652ms/epoch - 2ms/step\n",
      "Epoch 114/200\n",
      "341/341 - 1s - loss: 13621940715520.0000 - mae: 1806476.5000 - val_loss: 13855288721408.0000 - val_mae: 1847152.7500 - 652ms/epoch - 2ms/step\n",
      "Epoch 115/200\n",
      "341/341 - 1s - loss: 13615063105536.0000 - mae: 1784656.8750 - val_loss: 13841982291968.0000 - val_mae: 1867349.7500 - 645ms/epoch - 2ms/step\n",
      "Epoch 116/200\n",
      "341/341 - 1s - loss: 13612873678848.0000 - mae: 1791733.6250 - val_loss: 13835342708736.0000 - val_mae: 1858216.2500 - 645ms/epoch - 2ms/step\n",
      "Epoch 117/200\n",
      "341/341 - 1s - loss: 13612284379136.0000 - mae: 1777515.5000 - val_loss: 13839771893760.0000 - val_mae: 1884685.5000 - 639ms/epoch - 2ms/step\n",
      "Epoch 118/200\n",
      "341/341 - 1s - loss: 13607877214208.0000 - mae: 1795348.6250 - val_loss: 13830690177024.0000 - val_mae: 1875856.7500 - 647ms/epoch - 2ms/step\n",
      "Epoch 119/200\n",
      "341/341 - 1s - loss: 13609940811776.0000 - mae: 1786359.5000 - val_loss: 13818962903040.0000 - val_mae: 1882269.0000 - 678ms/epoch - 2ms/step\n",
      "Epoch 120/200\n",
      "341/341 - 1s - loss: 13603146039296.0000 - mae: 1783037.0000 - val_loss: 13824294912000.0000 - val_mae: 1891859.1250 - 639ms/epoch - 2ms/step\n",
      "Epoch 121/200\n",
      "341/341 - 1s - loss: 13600682934272.0000 - mae: 1796372.6250 - val_loss: 13828357095424.0000 - val_mae: 1872257.1250 - 652ms/epoch - 2ms/step\n",
      "Epoch 122/200\n",
      "341/341 - 1s - loss: 13595578466304.0000 - mae: 1781194.6250 - val_loss: 13824952369152.0000 - val_mae: 1880501.6250 - 638ms/epoch - 2ms/step\n",
      "Epoch 123/200\n",
      "341/341 - 1s - loss: 13591182835712.0000 - mae: 1791978.2500 - val_loss: 13819032109056.0000 - val_mae: 1878443.6250 - 651ms/epoch - 2ms/step\n",
      "Epoch 124/200\n",
      "341/341 - 1s - loss: 13584526475264.0000 - mae: 1774625.8750 - val_loss: 13819899281408.0000 - val_mae: 1905588.1250 - 674ms/epoch - 2ms/step\n",
      "Epoch 125/200\n",
      "341/341 - 1s - loss: 13590615556096.0000 - mae: 1786378.2500 - val_loss: 13809534107648.0000 - val_mae: 1894010.5000 - 656ms/epoch - 2ms/step\n",
      "Epoch 126/200\n",
      "341/341 - 1s - loss: 13592928714752.0000 - mae: 1796839.7500 - val_loss: 13815560273920.0000 - val_mae: 1908447.5000 - 643ms/epoch - 2ms/step\n",
      "Epoch 127/200\n",
      "341/341 - 1s - loss: 13589742092288.0000 - mae: 1794749.2500 - val_loss: 13801865871360.0000 - val_mae: 1888303.3750 - 697ms/epoch - 2ms/step\n",
      "Epoch 128/200\n",
      "341/341 - 1s - loss: 13570619211776.0000 - mae: 1795102.0000 - val_loss: 13795861725184.0000 - val_mae: 1849599.6250 - 672ms/epoch - 2ms/step\n",
      "Epoch 129/200\n",
      "341/341 - 1s - loss: 13578051518464.0000 - mae: 1781072.6250 - val_loss: 13789085827072.0000 - val_mae: 1852976.6250 - 665ms/epoch - 2ms/step\n",
      "Epoch 130/200\n",
      "341/341 - 1s - loss: 13579639062528.0000 - mae: 1780402.1250 - val_loss: 13781859041280.0000 - val_mae: 1873871.7500 - 660ms/epoch - 2ms/step\n",
      "Epoch 131/200\n",
      "341/341 - 1s - loss: 13570828926976.0000 - mae: 1782592.3750 - val_loss: 13785445171200.0000 - val_mae: 1883064.7500 - 650ms/epoch - 2ms/step\n",
      "Epoch 132/200\n",
      "341/341 - 1s - loss: 13563554955264.0000 - mae: 1792087.2500 - val_loss: 13787540226048.0000 - val_mae: 1875390.2500 - 669ms/epoch - 2ms/step\n",
      "Epoch 133/200\n",
      "341/341 - 1s - loss: 13561609846784.0000 - mae: 1776509.0000 - val_loss: 13794477604864.0000 - val_mae: 1889406.5000 - 648ms/epoch - 2ms/step\n",
      "Epoch 134/200\n",
      "341/341 - 1s - loss: 13561190416384.0000 - mae: 1794996.0000 - val_loss: 13778543443968.0000 - val_mae: 1877046.7500 - 664ms/epoch - 2ms/step\n",
      "Epoch 135/200\n",
      "341/341 - 1s - loss: 13545708191744.0000 - mae: 1795803.1250 - val_loss: 13779716800512.0000 - val_mae: 1838926.0000 - 663ms/epoch - 2ms/step\n",
      "Epoch 136/200\n",
      "341/341 - 1s - loss: 13552098213888.0000 - mae: 1773440.3750 - val_loss: 13773173686272.0000 - val_mae: 1875932.5000 - 658ms/epoch - 2ms/step\n",
      "Epoch 137/200\n",
      "341/341 - 1s - loss: 13548825608192.0000 - mae: 1790277.1250 - val_loss: 13760340164608.0000 - val_mae: 1852586.8750 - 662ms/epoch - 2ms/step\n",
      "Epoch 138/200\n",
      "341/341 - 1s - loss: 13545784737792.0000 - mae: 1780585.1250 - val_loss: 13760596017152.0000 - val_mae: 1869461.0000 - 648ms/epoch - 2ms/step\n",
      "Epoch 139/200\n",
      "341/341 - 1s - loss: 13540289150976.0000 - mae: 1778189.2500 - val_loss: 13756173123584.0000 - val_mae: 1887519.6250 - 649ms/epoch - 2ms/step\n",
      "Epoch 140/200\n",
      "341/341 - 1s - loss: 13537211580416.0000 - mae: 1783375.0000 - val_loss: 13742761836544.0000 - val_mae: 1882771.8750 - 657ms/epoch - 2ms/step\n",
      "Epoch 141/200\n",
      "341/341 - 1s - loss: 13528284004352.0000 - mae: 1792725.3750 - val_loss: 13735265566720.0000 - val_mae: 1868507.1250 - 643ms/epoch - 2ms/step\n",
      "Epoch 142/200\n",
      "341/341 - 1s - loss: 13523624132608.0000 - mae: 1770573.8750 - val_loss: 13736191459328.0000 - val_mae: 1899896.2500 - 696ms/epoch - 2ms/step\n",
      "Epoch 143/200\n",
      "341/341 - 1s - loss: 13528929927168.0000 - mae: 1780246.5000 - val_loss: 13723905294336.0000 - val_mae: 1888340.7500 - 653ms/epoch - 2ms/step\n",
      "Epoch 144/200\n",
      "341/341 - 1s - loss: 13515762958336.0000 - mae: 1780913.0000 - val_loss: 13740344868864.0000 - val_mae: 1909783.6250 - 657ms/epoch - 2ms/step\n",
      "Epoch 145/200\n",
      "341/341 - 1s - loss: 13511029686272.0000 - mae: 1794271.1250 - val_loss: 13718197895168.0000 - val_mae: 1858791.7500 - 663ms/epoch - 2ms/step\n",
      "Epoch 146/200\n",
      "341/341 - 1s - loss: 13504506494976.0000 - mae: 1784382.7500 - val_loss: 13715830210560.0000 - val_mae: 1854100.8750 - 676ms/epoch - 2ms/step\n",
      "Epoch 147/200\n",
      "341/341 - 1s - loss: 13504881885184.0000 - mae: 1768563.3750 - val_loss: 13707124932608.0000 - val_mae: 1883824.2500 - 678ms/epoch - 2ms/step\n",
      "Epoch 148/200\n",
      "341/341 - 1s - loss: 13501312532480.0000 - mae: 1786495.7500 - val_loss: 13700508418048.0000 - val_mae: 1869610.6250 - 693ms/epoch - 2ms/step\n",
      "Epoch 149/200\n",
      "341/341 - 1s - loss: 13499168194560.0000 - mae: 1787286.6250 - val_loss: 13692697575424.0000 - val_mae: 1846731.7500 - 643ms/epoch - 2ms/step\n",
      "Epoch 150/200\n",
      "341/341 - 1s - loss: 13487486009344.0000 - mae: 1768298.5000 - val_loss: 13685405777920.0000 - val_mae: 1881538.1250 - 646ms/epoch - 2ms/step\n",
      "Epoch 151/200\n",
      "341/341 - 1s - loss: 13484137906176.0000 - mae: 1784107.5000 - val_loss: 13680028680192.0000 - val_mae: 1881791.0000 - 670ms/epoch - 2ms/step\n",
      "Epoch 152/200\n",
      "341/341 - 1s - loss: 13479360593920.0000 - mae: 1774012.6250 - val_loss: 13670070353920.0000 - val_mae: 1871731.6250 - 651ms/epoch - 2ms/step\n",
      "Epoch 153/200\n",
      "341/341 - 1s - loss: 13474714353664.0000 - mae: 1774140.6250 - val_loss: 13674881220608.0000 - val_mae: 1893171.5000 - 650ms/epoch - 2ms/step\n",
      "Epoch 154/200\n",
      "341/341 - 1s - loss: 13464204476416.0000 - mae: 1785548.2500 - val_loss: 13645345980416.0000 - val_mae: 1866617.0000 - 641ms/epoch - 2ms/step\n",
      "Epoch 155/200\n",
      "341/341 - 1s - loss: 13459217448960.0000 - mae: 1777065.1250 - val_loss: 13649411309568.0000 - val_mae: 1863365.8750 - 650ms/epoch - 2ms/step\n",
      "Epoch 156/200\n",
      "341/341 - 1s - loss: 13457563844608.0000 - mae: 1772614.6250 - val_loss: 13644516556800.0000 - val_mae: 1878377.1250 - 653ms/epoch - 2ms/step\n",
      "Epoch 157/200\n",
      "341/341 - 1s - loss: 13449388097536.0000 - mae: 1780486.7500 - val_loss: 13637406162944.0000 - val_mae: 1860455.2500 - 655ms/epoch - 2ms/step\n",
      "Epoch 158/200\n",
      "341/341 - 1s - loss: 13444303552512.0000 - mae: 1775678.0000 - val_loss: 13629647749120.0000 - val_mae: 1881600.0000 - 672ms/epoch - 2ms/step\n",
      "Epoch 159/200\n",
      "341/341 - 1s - loss: 13439534628864.0000 - mae: 1781679.1250 - val_loss: 13609104048128.0000 - val_mae: 1854583.7500 - 651ms/epoch - 2ms/step\n",
      "Epoch 160/200\n",
      "341/341 - 1s - loss: 13433017729024.0000 - mae: 1770351.7500 - val_loss: 13606283378688.0000 - val_mae: 1875361.1250 - 687ms/epoch - 2ms/step\n",
      "Epoch 161/200\n",
      "341/341 - 1s - loss: 13415798013952.0000 - mae: 1765593.1250 - val_loss: 13611834540032.0000 - val_mae: 1901234.5000 - 653ms/epoch - 2ms/step\n",
      "Epoch 162/200\n",
      "341/341 - 1s - loss: 13420766167040.0000 - mae: 1776905.7500 - val_loss: 13605047107584.0000 - val_mae: 1902097.2500 - 639ms/epoch - 2ms/step\n",
      "Epoch 163/200\n",
      "341/341 - 1s - loss: 13411319545856.0000 - mae: 1789533.2500 - val_loss: 13582728167424.0000 - val_mae: 1836174.1250 - 649ms/epoch - 2ms/step\n",
      "Epoch 164/200\n",
      "341/341 - 1s - loss: 13402165477376.0000 - mae: 1773508.3750 - val_loss: 13568361627648.0000 - val_mae: 1848705.8750 - 704ms/epoch - 2ms/step\n",
      "Epoch 165/200\n",
      "341/341 - 1s - loss: 13394719539200.0000 - mae: 1751440.6250 - val_loss: 13582136770560.0000 - val_mae: 1898990.8750 - 658ms/epoch - 2ms/step\n",
      "Epoch 166/200\n",
      "341/341 - 1s - loss: 13391710126080.0000 - mae: 1780483.6250 - val_loss: 13565219045376.0000 - val_mae: 1876653.5000 - 660ms/epoch - 2ms/step\n",
      "Epoch 167/200\n",
      "341/341 - 1s - loss: 13380258627584.0000 - mae: 1786873.3750 - val_loss: 13548943048704.0000 - val_mae: 1836510.5000 - 642ms/epoch - 2ms/step\n",
      "Epoch 168/200\n",
      "341/341 - 1s - loss: 13374739972096.0000 - mae: 1758670.7500 - val_loss: 13540770447360.0000 - val_mae: 1877084.5000 - 657ms/epoch - 2ms/step\n",
      "Epoch 169/200\n",
      "341/341 - 1s - loss: 13367999725568.0000 - mae: 1779100.2500 - val_loss: 13525503180800.0000 - val_mae: 1858822.0000 - 659ms/epoch - 2ms/step\n",
      "Epoch 170/200\n",
      "341/341 - 1s - loss: 13358497529856.0000 - mae: 1769952.1250 - val_loss: 13511150272512.0000 - val_mae: 1853048.2500 - 655ms/epoch - 2ms/step\n",
      "Epoch 171/200\n",
      "341/341 - 1s - loss: 13349727240192.0000 - mae: 1763781.6250 - val_loss: 13502540414976.0000 - val_mae: 1855012.2500 - 688ms/epoch - 2ms/step\n",
      "Epoch 172/200\n",
      "341/341 - 1s - loss: 13341235871744.0000 - mae: 1759930.1250 - val_loss: 13508774199296.0000 - val_mae: 1889598.0000 - 647ms/epoch - 2ms/step\n",
      "Epoch 173/200\n",
      "341/341 - 1s - loss: 13334398107648.0000 - mae: 1783671.1250 - val_loss: 13488991764480.0000 - val_mae: 1847371.5000 - 651ms/epoch - 2ms/step\n",
      "Epoch 174/200\n",
      "341/341 - 1s - loss: 13329639669760.0000 - mae: 1757766.6250 - val_loss: 13481987276800.0000 - val_mae: 1865968.0000 - 653ms/epoch - 2ms/step\n",
      "Epoch 175/200\n",
      "341/341 - 1s - loss: 13323475091456.0000 - mae: 1776180.6250 - val_loss: 13461247492096.0000 - val_mae: 1849015.5000 - 660ms/epoch - 2ms/step\n",
      "Epoch 176/200\n",
      "341/341 - 1s - loss: 13311728943104.0000 - mae: 1750594.0000 - val_loss: 13465651511296.0000 - val_mae: 1884865.0000 - 638ms/epoch - 2ms/step\n",
      "Epoch 177/200\n",
      "341/341 - 1s - loss: 13309145251840.0000 - mae: 1775282.6250 - val_loss: 13450689380352.0000 - val_mae: 1838078.1250 - 679ms/epoch - 2ms/step\n",
      "Epoch 178/200\n",
      "341/341 - 1s - loss: 13294217723904.0000 - mae: 1763037.8750 - val_loss: 13435853078528.0000 - val_mae: 1851802.3750 - 639ms/epoch - 2ms/step\n",
      "Epoch 179/200\n",
      "341/341 - 1s - loss: 13297046781952.0000 - mae: 1764968.2500 - val_loss: 13421350223872.0000 - val_mae: 1848803.6250 - 657ms/epoch - 2ms/step\n",
      "Epoch 180/200\n",
      "341/341 - 1s - loss: 13278815191040.0000 - mae: 1754304.6250 - val_loss: 13422098907136.0000 - val_mae: 1886323.6250 - 642ms/epoch - 2ms/step\n",
      "Epoch 181/200\n",
      "341/341 - 1s - loss: 13280508641280.0000 - mae: 1770138.2500 - val_loss: 13395483951104.0000 - val_mae: 1847798.3750 - 642ms/epoch - 2ms/step\n",
      "Epoch 182/200\n",
      "341/341 - 1s - loss: 13263659073536.0000 - mae: 1770472.7500 - val_loss: 13389954809856.0000 - val_mae: 1825563.6250 - 649ms/epoch - 2ms/step\n",
      "Epoch 183/200\n",
      "341/341 - 1s - loss: 13256616837120.0000 - mae: 1763739.3750 - val_loss: 13386992582656.0000 - val_mae: 1823484.2500 - 644ms/epoch - 2ms/step\n",
      "Epoch 184/200\n",
      "341/341 - 1s - loss: 13255397343232.0000 - mae: 1754325.1250 - val_loss: 13375494946816.0000 - val_mae: 1842644.0000 - 636ms/epoch - 2ms/step\n",
      "Epoch 185/200\n",
      "341/341 - 1s - loss: 13239972790272.0000 - mae: 1766199.6250 - val_loss: 13361241653248.0000 - val_mae: 1840872.8750 - 675ms/epoch - 2ms/step\n",
      "Epoch 186/200\n",
      "341/341 - 1s - loss: 13235801554944.0000 - mae: 1747214.5000 - val_loss: 13348025401344.0000 - val_mae: 1860561.6250 - 644ms/epoch - 2ms/step\n",
      "Epoch 187/200\n",
      "341/341 - 1s - loss: 13223579353088.0000 - mae: 1767910.7500 - val_loss: 13331121307648.0000 - val_mae: 1843751.1250 - 652ms/epoch - 2ms/step\n",
      "Epoch 188/200\n",
      "341/341 - 1s - loss: 13218761146368.0000 - mae: 1762255.3750 - val_loss: 13320196194304.0000 - val_mae: 1826406.0000 - 650ms/epoch - 2ms/step\n",
      "Epoch 189/200\n",
      "341/341 - 1s - loss: 13212061794304.0000 - mae: 1753186.7500 - val_loss: 13310836604928.0000 - val_mae: 1848577.8750 - 745ms/epoch - 2ms/step\n",
      "Epoch 190/200\n",
      "341/341 - 1s - loss: 13205013266432.0000 - mae: 1749419.1250 - val_loss: 13302579068928.0000 - val_mae: 1862790.0000 - 669ms/epoch - 2ms/step\n",
      "Epoch 191/200\n",
      "341/341 - 1s - loss: 13197673234432.0000 - mae: 1758638.2500 - val_loss: 13298484379648.0000 - val_mae: 1866216.8750 - 662ms/epoch - 2ms/step\n",
      "Epoch 192/200\n",
      "341/341 - 1s - loss: 13183158845440.0000 - mae: 1759081.7500 - val_loss: 13279449579520.0000 - val_mae: 1847596.7500 - 641ms/epoch - 2ms/step\n",
      "Epoch 193/200\n",
      "341/341 - 1s - loss: 13172776894464.0000 - mae: 1756118.5000 - val_loss: 13265121837056.0000 - val_mae: 1854399.8750 - 651ms/epoch - 2ms/step\n",
      "Epoch 194/200\n",
      "341/341 - 1s - loss: 13165251264512.0000 - mae: 1757824.3750 - val_loss: 13256757346304.0000 - val_mae: 1853877.2500 - 656ms/epoch - 2ms/step\n",
      "Epoch 195/200\n",
      "341/341 - 1s - loss: 13153188446208.0000 - mae: 1749797.2500 - val_loss: 13246101716992.0000 - val_mae: 1865253.3750 - 648ms/epoch - 2ms/step\n",
      "Epoch 196/200\n",
      "341/341 - 1s - loss: 13143231168512.0000 - mae: 1756837.5000 - val_loss: 13220493393920.0000 - val_mae: 1834423.8750 - 657ms/epoch - 2ms/step\n",
      "Epoch 197/200\n",
      "341/341 - 1s - loss: 13132528353280.0000 - mae: 1750116.1250 - val_loss: 13216287555584.0000 - val_mae: 1837584.1250 - 650ms/epoch - 2ms/step\n",
      "Epoch 198/200\n",
      "341/341 - 1s - loss: 13129543057408.0000 - mae: 1751055.6250 - val_loss: 13194049355776.0000 - val_mae: 1833022.3750 - 659ms/epoch - 2ms/step\n",
      "Epoch 199/200\n",
      "341/341 - 1s - loss: 13122295300096.0000 - mae: 1756160.6250 - val_loss: 13182025334784.0000 - val_mae: 1816408.3750 - 691ms/epoch - 2ms/step\n",
      "Epoch 200/200\n",
      "341/341 - 1s - loss: 13108560003072.0000 - mae: 1739772.7500 - val_loss: 13173319008256.0000 - val_mae: 1843008.8750 - 649ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7540069f00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.fit(\n",
    "    X_train_scaled, y_train, \n",
    "    epochs=200, verbose=2, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11605472444416.0, 1681804.375]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model_mlp.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mlp = model_mlp.predict(X_test_scaled, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_SCORE: 0.582032, MAPE: 29.432545, MSE: 11605471856705.865234, RMSE: 3406680.474701, MAE: 1681804.058785\n"
     ]
    }
   ],
   "source": [
    "_ = regression_evaluation(y_test, pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BayesianOptimization MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(units1, units2, units3):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units1, activation='relu', input_shape=(X_train.shape[1:])),\n",
    "        tf.keras.layers.Dense(units2, activation='relu'),\n",
    "        tf.keras.layers.Dense(units3, activation='relu'),\n",
    "        # 회귀모델에서는 출력층에 활성화함수를 사용하지 않음\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='mean_squared_error', \n",
    "        metrics=['mae', 'mape']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: [11244976209920.0, 1637490.125, 28.7490177154541]\n",
      "Processing Time: 0:01:14.586101\n"
     ]
    }
   ],
   "source": [
    "# 임의 실행\n",
    "_start = datetime.now()\n",
    "model_bo_mlp = create_mlp_model(256, 256, 128)\n",
    "model_bo_mlp.fit(X_train_scaled, y_train, epochs=100, verbose=0)\n",
    "loss = model_bo_mlp.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Processing Time: {datetime.now() - _start}')\n",
    "# scaling 전에는 16.xx 였는데 13.xx로 떨어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_SCORE: 0.595015, MAPE: 28.749016, MSE: 11244975805035.013672, RMSE: 3353352.919845, MAE: 1637489.816711\n"
     ]
    }
   ],
   "source": [
    "pred_bo_mlp = model_bo_mlp.predict(X_test_scaled, verbose=0)\n",
    "_ = regression_evaluation(y_test, pred_bo_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_func_mlp(units1, units2, units3, epochs):\n",
    "    model = create_mlp_model(int(units1), int(units2), int(units3))\n",
    "    model.fit(X_train_scaled, y_train, epochs=int(epochs), verbose=0)\n",
    "    loss = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    return -loss[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds_mlp = {\n",
    "    'units1': (128, 512),\n",
    "    'units2': (64, 256),\n",
    "    'units3': (32, 128),\n",
    "    'epochs': (50, 200)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "optimizer_mlp = BayesianOptimization(\n",
    "    f=None,\n",
    "    pbounds=pbounds_mlp,\n",
    "    verbose=2,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "utility = UtilityFunction(\n",
    "    kind='ucb', # 탐색과 활용 사이의 균형 유지, 불확실한 지점을 더 많이 탐색\n",
    "    kappa=2.5,  # UCB전략에서 얼마의 불확실성을 고려할지 지정\n",
    "                # 값이 높으면 탐색을, 값이 낮으면 활용을 강조함\n",
    "    xi=0.0      # 탐색전략에서 사용되는 파라미터로 얼마나 큰 개선을 고려할지 조정\n",
    "                # 값이 높으면 큰 개선을 값이 낮으면 작은 개선을 탐색함    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 28.67093276977539\n"
     ]
    }
   ],
   "source": [
    "# 임의실행\n",
    "next_point = optimizer_mlp.suggest(utility)\n",
    "target = target_func_mlp(**next_point)\n",
    "print(f'MAPE: {-target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_mlp.register(params=next_point,target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  Index |  Target | Units1 | Units2 | Units3 | Epochs |     Time     |\n",
      "|--------+---------+--------+--------+--------+--------+--------------|\n",
      "|      1 | 30.2167 |    291 |     70 |     73 |     71 |  0:00:53.125 |\n",
      "|      2 | 29.0478 |    235 |    100 |     94 |    153 |  0:01:53.465 |\n",
      "|      3 | 27.3826 |    254 |    115 |     61 |    167 |  0:02:02.821 |\n",
      "|      4 | 28.5088 |    254 |    142 |     32 |    186 |  0:02:16.452 |\n",
      "|      5 | 29.9241 |    279 |     98 |     53 |    178 |  0:02:10.787 |\n",
      "|      6 | 31.0467 |    213 |    147 |    102 |     89 |  0:01:05.890 |\n",
      "|      7 | 29.3200 |    230 |    110 |     76 |    156 |  0:01:54.924 |\n",
      "|      8 | 28.5836 |    477 |    231 |     83 |     84 |  0:01:02.440 |\n",
      "|      9 | 28.0370 |    257 |    125 |     60 |    183 |  0:02:14.470 |\n",
      "|     10 | 29.3948 |    259 |    128 |     70 |    150 |  0:01:50.206 |\n",
      "|     11 | 27.8041 |    441 |    160 |     72 |    191 |  0:02:19.957 |\n",
      "|     12 | 28.4907 |    248 |    119 |     50 |    172 |  0:02:07.720 |\n",
      "|     13 | 28.4743 |    133 |    110 |     82 |    192 |  0:02:23.515 |\n",
      "|     14 | 29.0640 |    157 |    207 |    119 |    141 |  0:01:44.764 |\n",
      "|     15 | 29.7513 |    355 |    143 |    115 |    158 |  0:01:57.513 |\n",
      "|     16 | 28.7531 |    255 |    112 |     59 |    160 |  0:01:58.866 |\n",
      "|     17 | 29.1528 |    135 |    107 |     79 |    194 |  0:02:21.872 |\n",
      "|     18 | 27.8954 |    256 |    128 |     62 |    176 |  0:02:09.486 |\n",
      "|     19 | 28.6098 |    259 |    117 |     50 |    171 |  0:02:05.130 |\n",
      "|     20 | 29.0729 |    249 |    120 |     71 |    178 |  0:02:12.664 |\n",
      "|--------+---------+--------+--------+--------+--------+--------------|\n",
      "{'target': -27.382551193237305, 'params': {'epochs': 167.99823530764698, 'units1': 254.24641109570794, 'units2': 115.94041015344085, 'units3': 61.05298067782441}}\n",
      "Processing Time: 0:38:46.077767\n"
     ]
    }
   ],
   "source": [
    "_start = datetime.now()\n",
    "print('|  Index |  Target | Units1 | Units2 | Units3 | Epochs |     Time     |')\n",
    "print('|--------+---------+--------+--------+--------+--------+--------------|')\n",
    "\n",
    "for n_iter in range(20):\n",
    "    __start = datetime.now()\n",
    "    next_point = optimizer_mlp.suggest(utility)\n",
    "    target = target_func_mlp(**next_point)\n",
    "    optimizer_mlp.register(params=next_point,target=target)\n",
    "    print(\n",
    "        f'|{(n_iter+1):>7} |{-target:8.4f} |{int(next_point[\"units1\"]):>7} ' \n",
    "        f'|{int(next_point[\"units2\"]):>7} |{int(next_point[\"units3\"]):>7} '\n",
    "        f'|{int(next_point[\"epochs\"]):>7} |  {str(datetime.now()-__start)[:-3]} |' \n",
    "    )\n",
    "    \n",
    "print('|--------+---------+--------+--------+--------+--------+--------------|')\n",
    "print(optimizer_mlp.max)\n",
    "print(f'Processing Time: {datetime.now() - _start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/170\n",
      "426/426 - 2s - loss: 55965944643584.0000 - mae: 5224086.0000 - mape: 96.1455 - 2s/epoch - 4ms/step\n",
      "Epoch 2/170\n",
      "426/426 - 1s - loss: 23387919351808.0000 - mae: 2682958.2500 - mape: 46.7804 - 781ms/epoch - 2ms/step\n",
      "Epoch 3/170\n",
      "426/426 - 1s - loss: 15187058884608.0000 - mae: 2043727.1250 - mape: 36.9128 - 778ms/epoch - 2ms/step\n",
      "Epoch 4/170\n",
      "426/426 - 1s - loss: 14768262873088.0000 - mae: 1987633.5000 - mape: 35.6349 - 738ms/epoch - 2ms/step\n",
      "Epoch 5/170\n",
      "426/426 - 1s - loss: 14522269040640.0000 - mae: 1947012.7500 - mape: 34.3275 - 771ms/epoch - 2ms/step\n",
      "Epoch 6/170\n",
      "426/426 - 1s - loss: 14366454841344.0000 - mae: 1906336.2500 - mape: 33.2617 - 737ms/epoch - 2ms/step\n",
      "Epoch 7/170\n",
      "426/426 - 1s - loss: 14256957292544.0000 - mae: 1892350.7500 - mape: 32.6956 - 732ms/epoch - 2ms/step\n",
      "Epoch 8/170\n",
      "426/426 - 1s - loss: 14168116690944.0000 - mae: 1874905.0000 - mape: 32.2827 - 753ms/epoch - 2ms/step\n",
      "Epoch 9/170\n",
      "426/426 - 1s - loss: 14111848005632.0000 - mae: 1869566.6250 - mape: 31.9809 - 777ms/epoch - 2ms/step\n",
      "Epoch 10/170\n",
      "426/426 - 1s - loss: 14067152453632.0000 - mae: 1852590.0000 - mape: 31.4586 - 758ms/epoch - 2ms/step\n",
      "Epoch 11/170\n",
      "426/426 - 1s - loss: 14033756356608.0000 - mae: 1847504.0000 - mape: 31.2978 - 754ms/epoch - 2ms/step\n",
      "Epoch 12/170\n",
      "426/426 - 1s - loss: 13999684976640.0000 - mae: 1845119.1250 - mape: 31.1887 - 775ms/epoch - 2ms/step\n",
      "Epoch 13/170\n",
      "426/426 - 1s - loss: 13968372400128.0000 - mae: 1834015.7500 - mape: 30.8423 - 781ms/epoch - 2ms/step\n",
      "Epoch 14/170\n",
      "426/426 - 1s - loss: 13952776929280.0000 - mae: 1835880.8750 - mape: 30.8567 - 740ms/epoch - 2ms/step\n",
      "Epoch 15/170\n",
      "426/426 - 1s - loss: 13931690065920.0000 - mae: 1832128.8750 - mape: 30.7241 - 765ms/epoch - 2ms/step\n",
      "Epoch 16/170\n",
      "426/426 - 1s - loss: 13929187115008.0000 - mae: 1836076.1250 - mape: 30.8215 - 750ms/epoch - 2ms/step\n",
      "Epoch 17/170\n",
      "426/426 - 1s - loss: 13908415873024.0000 - mae: 1830881.7500 - mape: 30.6294 - 768ms/epoch - 2ms/step\n",
      "Epoch 18/170\n",
      "426/426 - 1s - loss: 13884654092288.0000 - mae: 1835670.3750 - mape: 30.7858 - 757ms/epoch - 2ms/step\n",
      "Epoch 19/170\n",
      "426/426 - 1s - loss: 13876899872768.0000 - mae: 1825398.1250 - mape: 30.5788 - 779ms/epoch - 2ms/step\n",
      "Epoch 20/170\n",
      "426/426 - 1s - loss: 13883024605184.0000 - mae: 1830103.0000 - mape: 30.6690 - 763ms/epoch - 2ms/step\n",
      "Epoch 21/170\n",
      "426/426 - 1s - loss: 13869616463872.0000 - mae: 1818695.8750 - mape: 30.1503 - 749ms/epoch - 2ms/step\n",
      "Epoch 22/170\n",
      "426/426 - 1s - loss: 13869516849152.0000 - mae: 1833845.8750 - mape: 30.6499 - 757ms/epoch - 2ms/step\n",
      "Epoch 23/170\n",
      "426/426 - 1s - loss: 13859725246464.0000 - mae: 1835010.1250 - mape: 30.7606 - 746ms/epoch - 2ms/step\n",
      "Epoch 24/170\n",
      "426/426 - 1s - loss: 13858766848000.0000 - mae: 1822359.5000 - mape: 30.3514 - 757ms/epoch - 2ms/step\n",
      "Epoch 25/170\n",
      "426/426 - 1s - loss: 13868468273152.0000 - mae: 1825304.7500 - mape: 30.4508 - 788ms/epoch - 2ms/step\n",
      "Epoch 26/170\n",
      "426/426 - 1s - loss: 13849015091200.0000 - mae: 1813559.2500 - mape: 30.2399 - 732ms/epoch - 2ms/step\n",
      "Epoch 27/170\n",
      "426/426 - 1s - loss: 13858591735808.0000 - mae: 1824092.1250 - mape: 30.3261 - 731ms/epoch - 2ms/step\n",
      "Epoch 28/170\n",
      "426/426 - 1s - loss: 13844064763904.0000 - mae: 1833009.6250 - mape: 30.6849 - 766ms/epoch - 2ms/step\n",
      "Epoch 29/170\n",
      "426/426 - 1s - loss: 13846260482048.0000 - mae: 1822608.3750 - mape: 30.2799 - 816ms/epoch - 2ms/step\n",
      "Epoch 30/170\n",
      "426/426 - 1s - loss: 13838535622656.0000 - mae: 1822692.1250 - mape: 30.2712 - 738ms/epoch - 2ms/step\n",
      "Epoch 31/170\n",
      "426/426 - 1s - loss: 13835430789120.0000 - mae: 1825317.1250 - mape: 30.4566 - 737ms/epoch - 2ms/step\n",
      "Epoch 32/170\n",
      "426/426 - 1s - loss: 13831854096384.0000 - mae: 1817004.3750 - mape: 30.2714 - 743ms/epoch - 2ms/step\n",
      "Epoch 33/170\n",
      "426/426 - 1s - loss: 13836867338240.0000 - mae: 1827974.3750 - mape: 30.4164 - 751ms/epoch - 2ms/step\n",
      "Epoch 34/170\n",
      "426/426 - 1s - loss: 13830311641088.0000 - mae: 1821890.1250 - mape: 30.2845 - 721ms/epoch - 2ms/step\n",
      "Epoch 35/170\n",
      "426/426 - 1s - loss: 13825065615360.0000 - mae: 1818164.1250 - mape: 30.1781 - 723ms/epoch - 2ms/step\n",
      "Epoch 36/170\n",
      "426/426 - 1s - loss: 13821488922624.0000 - mae: 1818503.8750 - mape: 30.2937 - 797ms/epoch - 2ms/step\n",
      "Epoch 37/170\n",
      "426/426 - 1s - loss: 13813117091840.0000 - mae: 1823262.2500 - mape: 30.3471 - 738ms/epoch - 2ms/step\n",
      "Epoch 38/170\n",
      "426/426 - 1s - loss: 13815267721216.0000 - mae: 1813169.2500 - mape: 30.1055 - 727ms/epoch - 2ms/step\n",
      "Epoch 39/170\n",
      "426/426 - 1s - loss: 13825971585024.0000 - mae: 1830865.5000 - mape: 30.6518 - 744ms/epoch - 2ms/step\n",
      "Epoch 40/170\n",
      "426/426 - 1s - loss: 13816799690752.0000 - mae: 1817710.7500 - mape: 30.2197 - 720ms/epoch - 2ms/step\n",
      "Epoch 41/170\n",
      "426/426 - 1s - loss: 13813242920960.0000 - mae: 1817809.8750 - mape: 30.2033 - 717ms/epoch - 2ms/step\n",
      "Epoch 42/170\n",
      "426/426 - 1s - loss: 13800570880000.0000 - mae: 1815915.5000 - mape: 30.1837 - 731ms/epoch - 2ms/step\n",
      "Epoch 43/170\n",
      "426/426 - 1s - loss: 13800804712448.0000 - mae: 1832391.0000 - mape: 30.5561 - 727ms/epoch - 2ms/step\n",
      "Epoch 44/170\n",
      "426/426 - 1s - loss: 13809913692160.0000 - mae: 1812144.8750 - mape: 30.0505 - 728ms/epoch - 2ms/step\n",
      "Epoch 45/170\n",
      "426/426 - 1s - loss: 13800002551808.0000 - mae: 1826812.2500 - mape: 30.5359 - 752ms/epoch - 2ms/step\n",
      "Epoch 46/170\n",
      "426/426 - 1s - loss: 13805160497152.0000 - mae: 1819313.7500 - mape: 30.3498 - 804ms/epoch - 2ms/step\n",
      "Epoch 47/170\n",
      "426/426 - 1s - loss: 13792089997312.0000 - mae: 1810420.0000 - mape: 30.0459 - 745ms/epoch - 2ms/step\n",
      "Epoch 48/170\n",
      "426/426 - 1s - loss: 13796807540736.0000 - mae: 1831493.5000 - mape: 30.6233 - 745ms/epoch - 2ms/step\n",
      "Epoch 49/170\n",
      "426/426 - 1s - loss: 13785135841280.0000 - mae: 1821481.5000 - mape: 30.2445 - 722ms/epoch - 2ms/step\n",
      "Epoch 50/170\n",
      "426/426 - 1s - loss: 13791839387648.0000 - mae: 1801459.3750 - mape: 29.7873 - 720ms/epoch - 2ms/step\n",
      "Epoch 51/170\n",
      "426/426 - 1s - loss: 13788345532416.0000 - mae: 1820151.0000 - mape: 30.3055 - 727ms/epoch - 2ms/step\n",
      "Epoch 52/170\n",
      "426/426 - 1s - loss: 13786427686912.0000 - mae: 1819469.2500 - mape: 30.2966 - 775ms/epoch - 2ms/step\n",
      "Epoch 53/170\n",
      "426/426 - 1s - loss: 13778083119104.0000 - mae: 1817999.8750 - mape: 30.4293 - 752ms/epoch - 2ms/step\n",
      "Epoch 54/170\n",
      "426/426 - 1s - loss: 13781574877184.0000 - mae: 1817592.0000 - mape: 30.1752 - 768ms/epoch - 2ms/step\n",
      "Epoch 55/170\n",
      "426/426 - 1s - loss: 13776937025536.0000 - mae: 1820960.5000 - mape: 30.4186 - 741ms/epoch - 2ms/step\n",
      "Epoch 56/170\n",
      "426/426 - 1s - loss: 13772097847296.0000 - mae: 1813887.7500 - mape: 30.1932 - 734ms/epoch - 2ms/step\n",
      "Epoch 57/170\n",
      "426/426 - 1s - loss: 13766332776448.0000 - mae: 1819233.0000 - mape: 30.5151 - 750ms/epoch - 2ms/step\n",
      "Epoch 58/170\n",
      "426/426 - 1s - loss: 13770511351808.0000 - mae: 1817965.1250 - mape: 30.1794 - 761ms/epoch - 2ms/step\n",
      "Epoch 59/170\n",
      "426/426 - 1s - loss: 13773109723136.0000 - mae: 1814897.5000 - mape: 30.1037 - 750ms/epoch - 2ms/step\n",
      "Epoch 60/170\n",
      "426/426 - 1s - loss: 13769542467584.0000 - mae: 1815463.7500 - mape: 30.2708 - 738ms/epoch - 2ms/step\n",
      "Epoch 61/170\n",
      "426/426 - 1s - loss: 13764329996288.0000 - mae: 1816683.5000 - mape: 30.0893 - 812ms/epoch - 2ms/step\n",
      "Epoch 62/170\n",
      "426/426 - 1s - loss: 13757337042944.0000 - mae: 1809397.7500 - mape: 30.0621 - 744ms/epoch - 2ms/step\n",
      "Epoch 63/170\n",
      "426/426 - 1s - loss: 13756255961088.0000 - mae: 1819316.5000 - mape: 30.3487 - 788ms/epoch - 2ms/step\n",
      "Epoch 64/170\n",
      "426/426 - 1s - loss: 13747181584384.0000 - mae: 1813235.2500 - mape: 30.2747 - 736ms/epoch - 2ms/step\n",
      "Epoch 65/170\n",
      "426/426 - 1s - loss: 13759521226752.0000 - mae: 1817736.0000 - mape: 30.2878 - 735ms/epoch - 2ms/step\n",
      "Epoch 66/170\n",
      "426/426 - 1s - loss: 13741747863552.0000 - mae: 1819162.1250 - mape: 30.4051 - 750ms/epoch - 2ms/step\n",
      "Epoch 67/170\n",
      "426/426 - 1s - loss: 13743880667136.0000 - mae: 1815632.0000 - mape: 30.2984 - 866ms/epoch - 2ms/step\n",
      "Epoch 68/170\n",
      "426/426 - 1s - loss: 13732472160256.0000 - mae: 1816786.1250 - mape: 30.3008 - 758ms/epoch - 2ms/step\n",
      "Epoch 69/170\n",
      "426/426 - 1s - loss: 13733717868544.0000 - mae: 1806059.1250 - mape: 29.9781 - 723ms/epoch - 2ms/step\n",
      "Epoch 70/170\n",
      "426/426 - 1s - loss: 13737407807488.0000 - mae: 1812050.6250 - mape: 30.2550 - 778ms/epoch - 2ms/step\n",
      "Epoch 71/170\n",
      "426/426 - 1s - loss: 13731744448512.0000 - mae: 1820074.8750 - mape: 30.5063 - 851ms/epoch - 2ms/step\n",
      "Epoch 72/170\n",
      "426/426 - 1s - loss: 13727188385792.0000 - mae: 1813693.7500 - mape: 30.1262 - 790ms/epoch - 2ms/step\n",
      "Epoch 73/170\n",
      "426/426 - 1s - loss: 13736926511104.0000 - mae: 1801978.6250 - mape: 29.9006 - 869ms/epoch - 2ms/step\n",
      "Epoch 74/170\n",
      "426/426 - 1s - loss: 13722487619584.0000 - mae: 1819217.6250 - mape: 30.4360 - 821ms/epoch - 2ms/step\n",
      "Epoch 75/170\n",
      "426/426 - 1s - loss: 13716862009344.0000 - mae: 1807007.6250 - mape: 30.0383 - 834ms/epoch - 2ms/step\n",
      "Epoch 76/170\n",
      "426/426 - 1s - loss: 13709720158208.0000 - mae: 1813770.0000 - mape: 30.3503 - 827ms/epoch - 2ms/step\n",
      "Epoch 77/170\n",
      "426/426 - 1s - loss: 13702574112768.0000 - mae: 1810671.3750 - mape: 30.2562 - 770ms/epoch - 2ms/step\n",
      "Epoch 78/170\n",
      "426/426 - 1s - loss: 13704302166016.0000 - mae: 1811890.2500 - mape: 30.2404 - 804ms/epoch - 2ms/step\n",
      "Epoch 79/170\n",
      "426/426 - 1s - loss: 13694894342144.0000 - mae: 1806189.6250 - mape: 29.9823 - 814ms/epoch - 2ms/step\n",
      "Epoch 80/170\n",
      "426/426 - 1s - loss: 13686220521472.0000 - mae: 1807814.7500 - mape: 30.1601 - 778ms/epoch - 2ms/step\n",
      "Epoch 81/170\n",
      "426/426 - 1s - loss: 13683358957568.0000 - mae: 1824371.3750 - mape: 30.6930 - 839ms/epoch - 2ms/step\n",
      "Epoch 82/170\n",
      "426/426 - 1s - loss: 13681336254464.0000 - mae: 1791525.3750 - mape: 29.6960 - 842ms/epoch - 2ms/step\n",
      "Epoch 83/170\n",
      "426/426 - 1s - loss: 13693400121344.0000 - mae: 1814003.1250 - mape: 30.2765 - 833ms/epoch - 2ms/step\n",
      "Epoch 84/170\n",
      "426/426 - 1s - loss: 13664066207744.0000 - mae: 1806619.0000 - mape: 30.1983 - 751ms/epoch - 2ms/step\n",
      "Epoch 85/170\n",
      "426/426 - 1s - loss: 13655802380288.0000 - mae: 1809925.0000 - mape: 30.2216 - 737ms/epoch - 2ms/step\n",
      "Epoch 86/170\n",
      "426/426 - 1s - loss: 13639060815872.0000 - mae: 1818754.2500 - mape: 30.5326 - 739ms/epoch - 2ms/step\n",
      "Epoch 87/170\n",
      "426/426 - 1s - loss: 13643897896960.0000 - mae: 1793481.6250 - mape: 29.8504 - 754ms/epoch - 2ms/step\n",
      "Epoch 88/170\n",
      "426/426 - 1s - loss: 13634320203776.0000 - mae: 1801547.7500 - mape: 30.0725 - 730ms/epoch - 2ms/step\n",
      "Epoch 89/170\n",
      "426/426 - 1s - loss: 13626586955776.0000 - mae: 1813570.8750 - mape: 30.4077 - 744ms/epoch - 2ms/step\n",
      "Epoch 90/170\n",
      "426/426 - 1s - loss: 13620545060864.0000 - mae: 1803913.0000 - mape: 30.1013 - 791ms/epoch - 2ms/step\n",
      "Epoch 91/170\n",
      "426/426 - 1s - loss: 13606307495936.0000 - mae: 1795920.1250 - mape: 29.9070 - 714ms/epoch - 2ms/step\n",
      "Epoch 92/170\n",
      "426/426 - 1s - loss: 13590821076992.0000 - mae: 1802137.7500 - mape: 30.1960 - 717ms/epoch - 2ms/step\n",
      "Epoch 93/170\n",
      "426/426 - 1s - loss: 13590590390272.0000 - mae: 1805250.5000 - mape: 30.2772 - 723ms/epoch - 2ms/step\n",
      "Epoch 94/170\n",
      "426/426 - 1s - loss: 13590179348480.0000 - mae: 1806795.7500 - mape: 30.2000 - 720ms/epoch - 2ms/step\n",
      "Epoch 95/170\n",
      "426/426 - 1s - loss: 13537815560192.0000 - mae: 1783018.7500 - mape: 29.7469 - 720ms/epoch - 2ms/step\n",
      "Epoch 96/170\n",
      "426/426 - 1s - loss: 13537412907008.0000 - mae: 1813454.1250 - mape: 30.2672 - 735ms/epoch - 2ms/step\n",
      "Epoch 97/170\n",
      "426/426 - 1s - loss: 13541729894400.0000 - mae: 1786839.1250 - mape: 29.8137 - 730ms/epoch - 2ms/step\n",
      "Epoch 98/170\n",
      "426/426 - 1s - loss: 13521689509888.0000 - mae: 1796679.3750 - mape: 30.1156 - 726ms/epoch - 2ms/step\n",
      "Epoch 99/170\n",
      "426/426 - 1s - loss: 13515261739008.0000 - mae: 1801584.2500 - mape: 30.2090 - 827ms/epoch - 2ms/step\n",
      "Epoch 100/170\n",
      "426/426 - 1s - loss: 13488614277120.0000 - mae: 1788290.8750 - mape: 29.7497 - 737ms/epoch - 2ms/step\n",
      "Epoch 101/170\n",
      "426/426 - 1s - loss: 13473873395712.0000 - mae: 1791676.7500 - mape: 30.1396 - 721ms/epoch - 2ms/step\n",
      "Epoch 102/170\n",
      "426/426 - 1s - loss: 13468346351616.0000 - mae: 1788288.1250 - mape: 29.8452 - 731ms/epoch - 2ms/step\n",
      "Epoch 103/170\n",
      "426/426 - 1s - loss: 13442085814272.0000 - mae: 1796389.7500 - mape: 30.1724 - 722ms/epoch - 2ms/step\n",
      "Epoch 104/170\n",
      "426/426 - 1s - loss: 13416629534720.0000 - mae: 1783355.7500 - mape: 29.7331 - 771ms/epoch - 2ms/step\n",
      "Epoch 105/170\n",
      "426/426 - 1s - loss: 13395683180544.0000 - mae: 1787423.5000 - mape: 29.9025 - 736ms/epoch - 2ms/step\n",
      "Epoch 106/170\n",
      "426/426 - 1s - loss: 13378466611200.0000 - mae: 1776900.0000 - mape: 29.7311 - 730ms/epoch - 2ms/step\n",
      "Epoch 107/170\n",
      "426/426 - 1s - loss: 13359927787520.0000 - mae: 1787909.6250 - mape: 30.0975 - 725ms/epoch - 2ms/step\n",
      "Epoch 108/170\n",
      "426/426 - 1s - loss: 13328105603072.0000 - mae: 1775773.1250 - mape: 29.5090 - 736ms/epoch - 2ms/step\n",
      "Epoch 109/170\n",
      "426/426 - 1s - loss: 13315385327616.0000 - mae: 1787543.5000 - mape: 30.0938 - 727ms/epoch - 2ms/step\n",
      "Epoch 110/170\n",
      "426/426 - 1s - loss: 13292134203392.0000 - mae: 1769209.2500 - mape: 29.6024 - 721ms/epoch - 2ms/step\n",
      "Epoch 111/170\n",
      "426/426 - 1s - loss: 13273675071488.0000 - mae: 1779452.0000 - mape: 29.8514 - 764ms/epoch - 2ms/step\n",
      "Epoch 112/170\n",
      "426/426 - 1s - loss: 13252204429312.0000 - mae: 1772330.5000 - mape: 29.6860 - 739ms/epoch - 2ms/step\n",
      "Epoch 113/170\n",
      "426/426 - 1s - loss: 13223803748352.0000 - mae: 1773329.1250 - mape: 29.8199 - 746ms/epoch - 2ms/step\n",
      "Epoch 114/170\n",
      "426/426 - 1s - loss: 13205524971520.0000 - mae: 1775068.0000 - mape: 29.8578 - 721ms/epoch - 2ms/step\n",
      "Epoch 115/170\n",
      "426/426 - 1s - loss: 13184937230336.0000 - mae: 1758339.8750 - mape: 29.4704 - 727ms/epoch - 2ms/step\n",
      "Epoch 116/170\n",
      "426/426 - 1s - loss: 13158338002944.0000 - mae: 1769339.0000 - mape: 29.7349 - 741ms/epoch - 2ms/step\n",
      "Epoch 117/170\n",
      "426/426 - 1s - loss: 13143981948928.0000 - mae: 1766433.7500 - mape: 29.6561 - 734ms/epoch - 2ms/step\n",
      "Epoch 118/170\n",
      "426/426 - 1s - loss: 13112114675712.0000 - mae: 1759996.7500 - mape: 29.6335 - 731ms/epoch - 2ms/step\n",
      "Epoch 119/170\n",
      "426/426 - 1s - loss: 13090652422144.0000 - mae: 1755281.1250 - mape: 29.3852 - 735ms/epoch - 2ms/step\n",
      "Epoch 120/170\n",
      "426/426 - 1s - loss: 13059203530752.0000 - mae: 1754056.6250 - mape: 29.4108 - 728ms/epoch - 2ms/step\n",
      "Epoch 121/170\n",
      "426/426 - 1s - loss: 13035895783424.0000 - mae: 1761798.0000 - mape: 29.7700 - 731ms/epoch - 2ms/step\n",
      "Epoch 122/170\n",
      "426/426 - 1s - loss: 13011489128448.0000 - mae: 1753965.1250 - mape: 29.5699 - 761ms/epoch - 2ms/step\n",
      "Epoch 123/170\n",
      "426/426 - 1s - loss: 12993879343104.0000 - mae: 1751030.8750 - mape: 29.4360 - 721ms/epoch - 2ms/step\n",
      "Epoch 124/170\n",
      "426/426 - 1s - loss: 12958823350272.0000 - mae: 1748115.5000 - mape: 29.4029 - 730ms/epoch - 2ms/step\n",
      "Epoch 125/170\n",
      "426/426 - 1s - loss: 12930811691008.0000 - mae: 1737552.3750 - mape: 29.2079 - 730ms/epoch - 2ms/step\n",
      "Epoch 126/170\n",
      "426/426 - 1s - loss: 12922709344256.0000 - mae: 1753764.7500 - mape: 29.6833 - 744ms/epoch - 2ms/step\n",
      "Epoch 127/170\n",
      "426/426 - 1s - loss: 12888997625856.0000 - mae: 1746489.0000 - mape: 29.4727 - 727ms/epoch - 2ms/step\n",
      "Epoch 128/170\n",
      "426/426 - 1s - loss: 12872956510208.0000 - mae: 1740393.3750 - mape: 29.1649 - 736ms/epoch - 2ms/step\n",
      "Epoch 129/170\n",
      "426/426 - 1s - loss: 12849935024128.0000 - mae: 1737199.1250 - mape: 29.3301 - 726ms/epoch - 2ms/step\n",
      "Epoch 130/170\n",
      "426/426 - 1s - loss: 12825166610432.0000 - mae: 1734626.3750 - mape: 29.2761 - 725ms/epoch - 2ms/step\n",
      "Epoch 131/170\n",
      "426/426 - 1s - loss: 12795579990016.0000 - mae: 1738596.8750 - mape: 29.3599 - 735ms/epoch - 2ms/step\n",
      "Epoch 132/170\n",
      "426/426 - 1s - loss: 12789025341440.0000 - mae: 1726038.8750 - mape: 29.0821 - 727ms/epoch - 2ms/step\n",
      "Epoch 133/170\n",
      "426/426 - 1s - loss: 12761435209728.0000 - mae: 1735954.3750 - mape: 29.3218 - 734ms/epoch - 2ms/step\n",
      "Epoch 134/170\n",
      "426/426 - 1s - loss: 12743438499840.0000 - mae: 1728788.3750 - mape: 29.1909 - 741ms/epoch - 2ms/step\n",
      "Epoch 135/170\n",
      "426/426 - 1s - loss: 12727599759360.0000 - mae: 1730374.0000 - mape: 29.2777 - 726ms/epoch - 2ms/step\n",
      "Epoch 136/170\n",
      "426/426 - 1s - loss: 12708162306048.0000 - mae: 1719120.3750 - mape: 28.9432 - 724ms/epoch - 2ms/step\n",
      "Epoch 137/170\n",
      "426/426 - 1s - loss: 12680434810880.0000 - mae: 1725935.6250 - mape: 29.2377 - 765ms/epoch - 2ms/step\n",
      "Epoch 138/170\n",
      "426/426 - 1s - loss: 12671777767424.0000 - mae: 1729509.1250 - mape: 29.2626 - 726ms/epoch - 2ms/step\n",
      "Epoch 139/170\n",
      "426/426 - 1s - loss: 12659781009408.0000 - mae: 1723701.2500 - mape: 29.1657 - 765ms/epoch - 2ms/step\n",
      "Epoch 140/170\n",
      "426/426 - 1s - loss: 12644275716096.0000 - mae: 1723452.6250 - mape: 29.2699 - 718ms/epoch - 2ms/step\n",
      "Epoch 141/170\n",
      "426/426 - 1s - loss: 12630403055616.0000 - mae: 1717275.8750 - mape: 28.9524 - 756ms/epoch - 2ms/step\n",
      "Epoch 142/170\n",
      "426/426 - 1s - loss: 12611809705984.0000 - mae: 1719442.8750 - mape: 29.0675 - 736ms/epoch - 2ms/step\n",
      "Epoch 143/170\n",
      "426/426 - 1s - loss: 12601173999616.0000 - mae: 1712211.3750 - mape: 28.9830 - 722ms/epoch - 2ms/step\n",
      "Epoch 144/170\n",
      "426/426 - 1s - loss: 12583515979776.0000 - mae: 1712222.8750 - mape: 28.8828 - 720ms/epoch - 2ms/step\n",
      "Epoch 145/170\n",
      "426/426 - 1s - loss: 12585538682880.0000 - mae: 1720549.7500 - mape: 29.1767 - 731ms/epoch - 2ms/step\n",
      "Epoch 146/170\n",
      "426/426 - 1s - loss: 12564719206400.0000 - mae: 1718632.2500 - mape: 29.3163 - 726ms/epoch - 2ms/step\n",
      "Epoch 147/170\n",
      "426/426 - 1s - loss: 12562950258688.0000 - mae: 1716776.8750 - mape: 29.0309 - 721ms/epoch - 2ms/step\n",
      "Epoch 148/170\n",
      "426/426 - 1s - loss: 12554571087872.0000 - mae: 1714114.3750 - mape: 29.0880 - 750ms/epoch - 2ms/step\n",
      "Epoch 149/170\n",
      "426/426 - 1s - loss: 12543963693056.0000 - mae: 1711991.7500 - mape: 29.0389 - 719ms/epoch - 2ms/step\n",
      "Epoch 150/170\n",
      "426/426 - 1s - loss: 12529327669248.0000 - mae: 1715992.2500 - mape: 29.0318 - 722ms/epoch - 2ms/step\n",
      "Epoch 151/170\n",
      "426/426 - 1s - loss: 12524014534656.0000 - mae: 1709190.1250 - mape: 29.0849 - 733ms/epoch - 2ms/step\n",
      "Epoch 152/170\n",
      "426/426 - 1s - loss: 12523472420864.0000 - mae: 1706379.2500 - mape: 28.8008 - 762ms/epoch - 2ms/step\n",
      "Epoch 153/170\n",
      "426/426 - 1s - loss: 12506779090944.0000 - mae: 1711633.0000 - mape: 29.0458 - 722ms/epoch - 2ms/step\n",
      "Epoch 154/170\n",
      "426/426 - 1s - loss: 12514465153024.0000 - mae: 1708696.1250 - mape: 29.0029 - 742ms/epoch - 2ms/step\n",
      "Epoch 155/170\n",
      "426/426 - 1s - loss: 12506682621952.0000 - mae: 1705667.1250 - mape: 28.9335 - 804ms/epoch - 2ms/step\n",
      "Epoch 156/170\n",
      "426/426 - 1s - loss: 12496360439808.0000 - mae: 1708735.3750 - mape: 28.9655 - 753ms/epoch - 2ms/step\n",
      "Epoch 157/170\n",
      "426/426 - 1s - loss: 12494792818688.0000 - mae: 1709688.3750 - mape: 29.0888 - 728ms/epoch - 2ms/step\n",
      "Epoch 158/170\n",
      "426/426 - 1s - loss: 12499741048832.0000 - mae: 1708721.8750 - mape: 28.9609 - 737ms/epoch - 2ms/step\n",
      "Epoch 159/170\n",
      "426/426 - 1s - loss: 12486187155456.0000 - mae: 1703112.8750 - mape: 28.9044 - 742ms/epoch - 2ms/step\n",
      "Epoch 160/170\n",
      "426/426 - 1s - loss: 12486519554048.0000 - mae: 1709375.6250 - mape: 28.8960 - 726ms/epoch - 2ms/step\n",
      "Epoch 161/170\n",
      "426/426 - 1s - loss: 12484131946496.0000 - mae: 1703370.2500 - mape: 29.0256 - 731ms/epoch - 2ms/step\n",
      "Epoch 162/170\n",
      "426/426 - 1s - loss: 12476043231232.0000 - mae: 1713292.7500 - mape: 29.1614 - 742ms/epoch - 2ms/step\n",
      "Epoch 163/170\n",
      "426/426 - 1s - loss: 12479487803392.0000 - mae: 1702654.2500 - mape: 28.9138 - 727ms/epoch - 2ms/step\n",
      "Epoch 164/170\n",
      "426/426 - 1s - loss: 12474727268352.0000 - mae: 1707975.6250 - mape: 29.1199 - 729ms/epoch - 2ms/step\n",
      "Epoch 165/170\n",
      "426/426 - 1s - loss: 12461802520576.0000 - mae: 1706839.0000 - mape: 28.9795 - 744ms/epoch - 2ms/step\n",
      "Epoch 166/170\n",
      "426/426 - 1s - loss: 12468170522624.0000 - mae: 1702918.8750 - mape: 28.9705 - 732ms/epoch - 2ms/step\n",
      "Epoch 167/170\n",
      "426/426 - 1s - loss: 12458993385472.0000 - mae: 1703598.7500 - mape: 28.8996 - 717ms/epoch - 2ms/step\n",
      "Epoch 168/170\n",
      "426/426 - 1s - loss: 12465108680704.0000 - mae: 1701598.5000 - mape: 28.8441 - 734ms/epoch - 2ms/step\n",
      "Epoch 169/170\n",
      "426/426 - 1s - loss: 12463851438080.0000 - mae: 1698564.0000 - mape: 28.9031 - 718ms/epoch - 2ms/step\n",
      "Epoch 170/170\n",
      "426/426 - 1s - loss: 12456682323968.0000 - mae: 1705240.0000 - mape: 28.9402 - 716ms/epoch - 2ms/step\n",
      "R2_SCORE: 0.596010, MAPE: 29.369218, MSE: 11217357212116.593750, RMSE: 3349232.331762, MAE: 1650757.854690\n"
     ]
    }
   ],
   "source": [
    "model_bo_mlp = create_mlp_model(256, 128, 64)\n",
    "model_bo_mlp.fit(X_train_scaled, y_train, epochs=170, verbose=2)\n",
    "pred_bo_mlp = model_bo_mlp.predict(X_test_scaled, verbose=0)\n",
    "_ = regression_evaluation(y_test, pred_bo_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t213p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
